#!/usr/bin/env python3
"""
ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨ - åŸºäºAgentLaceåè®®
å……åˆ†åˆ©ç”¨RTX 4090 GPUæ€§èƒ½ï¼Œæä¾›é«˜æ•ˆçš„ACTæ¨¡å‹æ¨ç†æœåŠ¡
"""

import sys
import os
import time
import argparse
import logging
import traceback
import base64
import numpy as np
import cv2
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ è·¯å¾„
current_dir = Path(__file__).parent
robot_server_dir = current_dir.parent
communication_dir = robot_server_dir.parent / "communication"

sys.path.append(str(robot_server_dir))
sys.path.append(str(communication_dir))
sys.path.append(str(communication_dir / "agentlace"))

# å¯¼å…¥AgentLace
from agentlace.trainer import TrainerServer, TrainerConfig

# å¯¼å…¥æ¨ç†å¼•æ“
from inference.act_inference import ACTInferenceEngine

class ACTDistributedServer:
    """åŸºäºAgentLaceçš„ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨"""
    
    def __init__(self, port: int = 5555, model_path: Optional[str] = None, log_level: str = "INFO"):
        self.port = port
        self.model_path = model_path
        self.server = None
        self.inference_engine = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.start_time = time.time()
        self.request_count = 0
        self.error_count = 0
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=getattr(logging, log_level.upper()))
        self.logger = logging.getLogger(__name__)
        
        print(f"ğŸ¤– ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨åˆå§‹åŒ–")
        print(f"   ç«¯å£: {port}")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path or 'é»˜è®¤è·¯å¾„'}")
    
    def start(self) -> bool:
        """å¯åŠ¨æ¨ç†æœåŠ¡å™¨"""
        try:
            print("ğŸš€ å¯åŠ¨ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨...")
            
            # 1. åˆå§‹åŒ–æ¨ç†å¼•æ“
            print("ğŸ¤– åˆå§‹åŒ–ACTæ¨ç†å¼•æ“...")
            self.inference_engine = ACTInferenceEngine(self.model_path, device="cuda")
            
            if not self.inference_engine.load_model():
                print("âŒ ACTæ¨¡å‹åŠ è½½å¤±è´¥")
                return False
            
            # 2. åˆ›å»ºAgentLaceé…ç½®
            config = TrainerConfig(
                port_number=self.port,
                broadcast_port=self.port + 1,
                request_types=["inference", "server_status"],
                rate_limit=1000,
                version="0.0.2"
            )
            
            # 3. åˆ›å»ºAgentLaceæœåŠ¡å™¨
            print("ğŸŒ åˆ›å»ºAgentLaceæœåŠ¡å™¨...")
            self.server = TrainerServer(
                config=config,
                request_callback=self._handle_request
            )
            
            print("âœ… ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ!")
            print(f"   æœåŠ¡åœ°å€: 10.16.49.124:{self.port}")
            print(f"   åè®®ç‰ˆæœ¬: AgentLace v{config.version}")
            print("ğŸ¯ ç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
            
            # 4. å¯åŠ¨æœåŠ¡å™¨ï¼ˆé˜»å¡æ¨¡å¼ï¼‰
            self.server.start()
            
            # ä¿æŒæœåŠ¡å™¨è¿è¡Œ
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·...")
                self.stop()
                
            return True
            
        except Exception as e:
            print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _handle_request(self, request_type: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†AgentLaceè¯·æ±‚"""
        if request_type == "inference":
            return self._handle_inference_request(payload)
        elif request_type == "server_status":
            return self._handle_status_request(payload)
        else:
            return {"success": False, "error": f"Unknown request type: {request_type}"}
    
    def _handle_inference_request(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ¨ç†è¯·æ±‚"""
        start_time = time.time()
        self.request_count += 1
        request_id = payload.get("request_id", f"req_{self.request_count}")
        
        try:
            # è§£æè¾“å…¥æ•°æ®
            qpos = np.array(payload["qpos"], dtype=np.float32)
            qvel = np.array(payload.get("qvel", [0.0] * 8), dtype=np.float32)
            
            # è§£ç å›¾åƒ
            image_b64 = payload["image"]
            image_bytes = base64.b64decode(image_b64)
            image_array = np.frombuffer(image_bytes, dtype=np.uint8)
            image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            print(f"ğŸ“¨ æ”¶åˆ°æ¨ç†è¯·æ±‚ #{self.request_count}")
            print(f"   qpos: {qpos.shape}, èŒƒå›´: [{qpos.min():.3f}, {qpos.max():.3f}]")
            print(f"   image: {image.shape}")
            
            # æ‰§è¡Œæ¨ç†
            actions, inference_time = self.inference_engine.predict(qpos, image)
            
            # è®¡ç®—æ€»å»¶è¿Ÿ
            total_latency = (time.time() - start_time) * 1000
            
            print(f"âœ… æ¨ç†å®Œæˆ")
            print(f"   æ¨ç†è€—æ—¶: {inference_time:.2f}ms")
            print(f"   æ€»ä½“å»¶è¿Ÿ: {total_latency:.2f}ms")
            print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min():.3f}, {actions.max():.3f}]")
            
            return {
                "success": True,
                "actions": actions.tolist(),
                "latency_ms": total_latency,
                "inference_ms": inference_time,
                "request_id": request_id
            }
            
        except Exception as e:
            self.error_count += 1
            error_msg = f"æ¨ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            traceback.print_exc()
            
            return {
                "success": False,
                "error_msg": error_msg,
                "latency_ms": (time.time() - start_time) * 1000,
                "request_id": request_id
            }
    
    def _handle_status_request(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†çŠ¶æ€æŸ¥è¯¢è¯·æ±‚"""
        try:
            uptime = time.time() - self.start_time
            engine_stats = self.inference_engine.get_stats() if self.inference_engine else {}
            
            status = {
                "success": True,
                "status": "running",
                "uptime_seconds": uptime,
                "request_count": self.request_count,
                "error_count": self.error_count,
                "success_rate": (self.request_count - self.error_count) / max(self.request_count, 1),
                "engine_stats": engine_stats
            }
            
            print(f"ğŸ“Š çŠ¶æ€æŸ¥è¯¢: {self.request_count}æ¬¡è¯·æ±‚, {self.error_count}æ¬¡é”™è¯¯")
            return status
            
        except Exception as e:
            return {"success": False, "error": str(e)}

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨")
    parser.add_argument("--port", type=int, default=5555,
                       help="æœåŠ¡ç«¯å£ (é»˜è®¤: 5555)")
    parser.add_argument("--model_path", type=str, default=None,
                       help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨æŸ¥æ‰¾)")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡å™¨
    server = ACTDistributedServer(
        port=args.port,
        model_path=args.model_path,
        log_level=args.log_level
    )
    
    try:
        server.start()
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡å™¨...")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨è¿è¡Œé”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        print("ğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    main()
