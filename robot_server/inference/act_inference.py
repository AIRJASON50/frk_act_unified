#!/usr/bin/env python3
"""
ACTæ¨¡å‹æ¨ç†æ¨¡å—
åŸºäºè®­ç»ƒå¥½çš„Frankaå•è‡‚ACTæ¨¡å‹è¿›è¡Œæ¨ç†
"""

import torch
import torchvision.transforms as transforms
import numpy as np
import pickle
import time
import os
import sys
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import traceback

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ACTç»„ä»¶
current_dir = Path(__file__).parent
robot_server_dir = current_dir.parent
sys.path.append(str(robot_server_dir))

from act_algo_train.policy import ACTPolicy
from act_algo_train.franka_constants import STATE_DIM, ACTION_DIM, CHUNK_SIZE

# å¸¸é‡å®šä¹‰
class ACTConstants:
    STATE_DIM = 8
    ACTION_DIM = 8  
    CHUNK_SIZE = 100
    IMAGE_HEIGHT = 480
    IMAGE_WIDTH = 640

class ACTConfig:
    @staticmethod
    def get_model_config():
        return {
            'lr': 1e-5,
            'num_queries': 100,
            'kl_weight': 10,
            'hidden_dim': 512,
            'dim_feedforward': 3200,
            'lr_backbone': 1e-5,
            'backbone': 'resnet18',
            'enc_layers': 4,
            'dec_layers': 7,
            'nheads': 8,
            'camera_names': ['top']
        }
    
    @staticmethod
    def get_default_model_path():
        return "/home/wujielin/CascadeProjects/data/act_training/models/checkpoints/franka_pick_place"

class ACTInferenceEngine:
    """ACTæ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: Optional[str] = None, device: str = "auto"):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            device: æ¨ç†è®¾å¤‡ï¼Œ"auto"è‡ªåŠ¨é€‰æ‹©ï¼Œ"cuda"æˆ–"cpu"
        """
        self.model = None
        self.device = self._setup_device(device)
        self.model_path = model_path or ACTConfig.get_default_model_path()
        self.is_loaded = False
        
        # æ•°æ®æ ‡å‡†åŒ–å‚æ•°
        self.stats = None
        self.pre_process = None
        self.post_process = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        print(f"ğŸ¤– ACTæ¨ç†å¼•æ“åˆå§‹åŒ–")
        print(f"   æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"   æ¨ç†è®¾å¤‡: {self.device}")
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®æ¨ç†è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                print(f"ğŸš€ æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                device = "cpu"
                print(f"ğŸ’» ä½¿ç”¨CPUæ¨ç†")
        
        return torch.device(device)
    
    def load_model(self) -> bool:
        """
        åŠ è½½ACTæ¨¡å‹
        
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ“¦ å¼€å§‹åŠ è½½ACTæ¨¡å‹...")
            
            # æŸ¥æ‰¾æœ€ä½³çš„æ£€æŸ¥ç‚¹
            model_file = self._find_best_checkpoint("best")
            if model_file is None:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_path}")
            
            print(f"ğŸ“„ ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: {model_file.name}")
            
            # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©ä¿¡æ¯
            self._print_model_info(model_file)
            
            # åˆ›å»ºæ¨¡å‹é…ç½®
            config = ACTConfig.get_model_config()
            
            # åˆ›å»ºç­–ç•¥æ¨¡å‹
            self.model = ACTPolicy(config)
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(model_file, map_location=self.device)
            
            # æ£€æŸ¥checkpointæ ¼å¼å¹¶åŠ è½½æƒé‡
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"ğŸ“Š æ£€æŸ¥ç‚¹æ ¼å¼: æ ‡å‡†æ ¼å¼")
            else:
                # ç›´æ¥æ˜¯æ¨¡å‹æƒé‡
                self.model.load_state_dict(checkpoint)
                print(f"ğŸ“Š æ£€æŸ¥ç‚¹æ ¼å¼: æƒé‡æ ¼å¼")
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åˆ°æŒ‡å®šè®¾å¤‡
            self.model.eval()
            if hasattr(self.model, 'to'):
                self.model = self.model.to(self.device)
            
            # åŠ è½½æ•°æ®ç»Ÿè®¡ä¿¡æ¯å¹¶è®¾ç½®æ ‡å‡†åŒ–å‡½æ•°
            if not self._load_dataset_stats():
                print("âš ï¸  æœªæ‰¾åˆ°æ•°æ®ç»Ÿè®¡æ–‡ä»¶ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®")
            
            # éªŒè¯æ¨¡å‹
            self._validate_model()
            
            self.is_loaded = True
            print(f"âœ… ACTæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   å‚æ•°æ•°é‡: {self._count_parameters():.2f}M")
            print(f"   è¾“å…¥ç»´åº¦: qpos({STATE_DIM}) + image({ACTConstants.IMAGE_HEIGHT}Ã—{ACTConstants.IMAGE_WIDTH}Ã—3)")
            print(f"   è¾“å‡ºç»´åº¦: actions({CHUNK_SIZE}Ã—{ACTION_DIM})")
            print(f"   è®¾å¤‡: {self.device}")
            print(f"   å›¾åƒé¢„å¤„ç†: ImageNetæ ‡å‡†åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰")
            norm_status = "å·²å¯ç”¨" if self.stats is not None else "æœªå¯ç”¨"
            print(f"   æ•°æ®æ ‡å‡†åŒ–: {norm_status} âœ¨")
            
            # æ‰“å°å¯ç”¨æ¨¡å‹åˆ—è¡¨
            self._print_available_models()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _find_best_checkpoint(self, selection_mode: str = "best") -> Optional[Path]:
        """æŸ¥æ‰¾æœ€ä½³æ£€æŸ¥ç‚¹æ–‡ä»¶
        
        Args:
            selection_mode: é€‰æ‹©æ¨¡å¼
                - "best": ä¼˜å…ˆé€‰æ‹©policy_best.ckptï¼Œå¦‚ä¸å­˜åœ¨åˆ™é€‰æ‹©æœ€æ–°epoch
                - "latest": é€‰æ‹©æœ€æ–°epochçš„æ¨¡å‹
                - "specific": ä½¿ç”¨å®Œæ•´è·¯å¾„æŒ‡å®šçš„æ¨¡å‹æ–‡ä»¶
        """
        model_path = Path(self.model_path)
        
        if model_path.is_file():
            print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ¨¡å‹æ–‡ä»¶: {model_path.name}")
            return model_path
        elif model_path.is_dir():
            print(f"ğŸ“ åœ¨ç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹: {model_path}")
            
            if selection_mode == "best":
                # ä¼˜å…ˆæŸ¥æ‰¾bestæ¨¡å‹
                best_file = model_path / "policy_best.ckpt"
                if best_file.exists():
                    print(f"âœ… æ‰¾åˆ°æœ€ä½³æ¨¡å‹: policy_best.ckpt (è®­ç»ƒè¿‡ç¨‹ä¸­éªŒè¯æŸå¤±æœ€å°)")
                    return best_file
                
                # å¦‚æœæ²¡æœ‰bestï¼ŒæŸ¥æ‰¾last
                last_file = model_path / "policy_last.ckpt"
                if last_file.exists():
                    print(f"âœ… æ‰¾åˆ°æœ€ç»ˆæ¨¡å‹: policy_last.ckpt (æœ€åä¸€è½®è®­ç»ƒ)")
                    return last_file
            
            # æŸ¥æ‰¾epochæ¨¡å‹æ–‡ä»¶
            ckpt_files = list(model_path.glob('policy_epoch_*.ckpt'))
            if not ckpt_files:
                print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
                return None
            
            # æŒ‰epochæ’åº
            ckpt_files.sort(key=lambda x: int(x.stem.split('_')[2]))
            
            if selection_mode == "latest" or selection_mode == "best":
                selected_file = ckpt_files[-1]
                epoch_num = int(selected_file.stem.split('_')[2])
                print(f"âœ… é€‰æ‹©æœ€æ–°epochæ¨¡å‹: {selected_file.name} (epoch {epoch_num})")
                return selected_file
                
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return None
    
    def _find_latest_checkpoint(self) -> Optional[Path]:
        """å‘åå…¼å®¹çš„æ–¹æ³•"""
        return self._find_best_checkpoint("latest")
    
    def _load_dataset_stats(self) -> bool:
        """åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ç”¨äºæ•°æ®æ ‡å‡†åŒ–"""
        try:
            # æŸ¥æ‰¾ç»Ÿè®¡æ–‡ä»¶
            model_dir = Path(self.model_path)
            if model_dir.is_file():
                model_dir = model_dir.parent
            
            stats_file = model_dir / "dataset_stats.pkl"
            if not stats_file.exists():
                print(f"âš ï¸  ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨: {stats_file}")
                return False
            
            # åŠ è½½ç»Ÿè®¡æ•°æ®
            with open(stats_file, 'rb') as f:
                self.stats = pickle.load(f)
            
            # è®¾ç½®æ ‡å‡†åŒ–å‡½æ•°
            qpos_mean = self.stats['qpos_mean']
            qpos_std = self.stats['qpos_std']
            action_mean = self.stats['action_mean']
            action_std = self.stats['action_std']
            
            self.pre_process = lambda qpos: (qpos - qpos_mean) / qpos_std
            self.post_process = lambda action: action * action_std + action_mean
            
            print(f"âœ… æ•°æ®ç»Ÿè®¡åŠ è½½æˆåŠŸ")
            print(f"   qpos_mean: {qpos_mean[:4]}... (8D)")
            print(f"   qpos_std: {qpos_std[:4]}... (8D)")
            print(f"   action_mean: {action_mean[:4]}... (8D)")
            print(f"   action_std: {action_std[:4]}... (8D)")
            print(f"   å¤¹çˆªåŠ¨ä½œèŒƒå›´: mean={action_mean[7]:.3f}, std={action_std[7]:.3f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            return False
    
    def _validate_model(self):
        """éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½"""
        print(f"ğŸ§ª éªŒè¯æ¨¡å‹åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 1
        qpos = torch.randn(batch_size, STATE_DIM).to(self.device)
        
        # åˆ›å»ºæ ‡å‡†åŒ–çš„å›¾åƒæ•°æ®ï¼ˆæ¨¡æ‹Ÿå®é™…è¾“å…¥ï¼‰
        test_image = torch.randn(batch_size, 1, 3, ACTConstants.IMAGE_HEIGHT, ACTConstants.IMAGE_WIDTH).to(self.device)
        # åº”ç”¨ImageNetæ ‡å‡†åŒ–
        normalize_transform = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        for b in range(batch_size):
            for c in range(1):  # cameraæ•°é‡
                test_image[b, c] = normalize_transform(torch.rand(3, ACTConstants.IMAGE_HEIGHT, ACTConstants.IMAGE_WIDTH).to(self.device))
        
        # æµ‹è¯•æ¨ç†
        with torch.no_grad():
            actions = self.model(qpos, test_image)
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        expected_shape = (batch_size, CHUNK_SIZE, ACTION_DIM)
        if actions.shape != expected_shape:
            raise RuntimeError(f"æ¨¡å‹è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{actions.shape}")
        
        # æ£€æŸ¥è¾“å‡ºèŒƒå›´æ˜¯å¦åˆç†
        action_range = [actions.min().item(), actions.max().item()]
        print(f"âœ… æ¨¡å‹éªŒè¯é€šè¿‡: è¾“å‡ºå½¢çŠ¶{actions.shape}, åŠ¨ä½œèŒƒå›´{action_range}")
        
        # å¦‚æœåŠ¨ä½œèŒƒå›´å¼‚å¸¸ï¼Œç»™å‡ºè­¦å‘Š
        if abs(action_range[0]) > 10 or abs(action_range[1]) > 10:
            print(f"âš ï¸ æ¨¡å‹è¾“å‡ºèŒƒå›´å¼‚å¸¸ï¼Œå¯èƒ½å­˜åœ¨é¢„å¤„ç†é—®é¢˜")
    
    def _print_model_info(self, model_file: Path):
        """æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        try:
            # è·å–æ–‡ä»¶å¤§å°
            file_size_mb = model_file.stat().st_size / (1024 * 1024)
            print(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.1f}MB")
            
            # å¦‚æœæ˜¯epochæ¨¡å‹ï¼Œæå–epochä¿¡æ¯
            if "epoch" in model_file.stem:
                epoch_num = int(model_file.stem.split('_')[2])
                print(f"   è®­ç»ƒè½®æ¬¡: Epoch {epoch_num}")
            elif "best" in model_file.stem:
                print(f"   æ¨¡å‹ç±»å‹: éªŒè¯æŸå¤±æœ€å°çš„æ¨¡å‹ â­ æ¨è")
            elif "last" in model_file.stem:
                print(f"   æ¨¡å‹ç±»å‹: æœ€ç»ˆè®­ç»ƒæ¨¡å‹")
                
        except Exception as e:
            print(f"   ä¿¡æ¯è·å–å¤±è´¥: {e}")
    
    def _print_available_models(self):
        """æ‰“å°å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            model_dir = Path(self.model_path)
            if model_dir.is_dir():
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
                
                # æŸ¥æ‰¾bestå’Œlastæ¨¡å‹
                special_files = ["policy_best.ckpt", "policy_last.ckpt"]
                for filename in special_files:
                    file_path = model_dir / filename
                    if file_path.exists():
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        print(f"   ğŸ“„ {filename} ({size_mb:.1f}MB)")
                
                # æŸ¥æ‰¾epochæ¨¡å‹
                epoch_files = sorted(model_dir.glob('policy_epoch_*.ckpt'), 
                                   key=lambda x: int(x.stem.split('_')[2]))
                if epoch_files:
                    print(f"   ğŸ“„ Epochæ¨¡å‹: {len(epoch_files)}ä¸ª (Epoch {int(epoch_files[0].stem.split('_')[2])}-{int(epoch_files[-1].stem.split('_')[2])})")
                    
        except Exception as e:
            print(f"   æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {e}")
    
    def _count_parameters(self) -> float:
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡(M)"""
        if self.model is None:
            return 0.0
        
        total_params = sum(p.numel() for p in self.model.parameters())
        return total_params / 1e6
    
    def predict(self, qpos: np.ndarray, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        æ‰§è¡ŒACTæ¨ç†
        
        Args:
            qpos: æœºå™¨äººå…³èŠ‚ä½ç½® (8,)
            image: RGBå›¾åƒ (H, W, 3)
            
        Returns:
            Tuple[np.ndarray, float]: (åŠ¨ä½œåºåˆ—(100, 8), æ¨ç†è€—æ—¶(ms))
        """
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        start_time = time.time()
        
        try:
            # æ•°æ®é¢„å¤„ç†
            preprocess_start = time.time()
            qpos_tensor, image_tensor = self._preprocess_input(qpos, image)
            preprocess_time = (time.time() - preprocess_start) * 1000
            
            # è°ƒè¯•ä¿¡æ¯ï¼šè¾“å…¥æ•°æ®åˆ†æ
            print(f"ğŸ” è¾“å…¥æ•°æ®åˆ†æ:")
            print(f"   qposåŸå§‹: shape{qpos.shape}, èŒƒå›´[{qpos.min():.3f}, {qpos.max():.3f}]")
            print(f"   qposå¼ é‡: shape{qpos_tensor.shape}, device={qpos_tensor.device}")
            print(f"   imageåŸå§‹: shape{image.shape}, dtype={image.dtype}, åƒç´ èŒƒå›´[{image.min()}, {image.max()}]")
            print(f"   imageå¼ é‡: shape{image_tensor.shape}, device={image_tensor.device}")
            print(f"   imageæ ‡å‡†åŒ–: ä½¿ç”¨ImageNetæ ‡å‡†åŒ– (mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])")
            print(f"   é¢„å¤„ç†è€—æ—¶: {preprocess_time:.2f}ms")
            
            # æ¨ç†
            model_start = time.time()
            with torch.no_grad():
                actions_tensor = self.model(qpos_tensor, image_tensor)
            model_time = (time.time() - model_start) * 1000
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ¨¡å‹è¾“å‡ºåˆ†æ
            print(f"ğŸ¤– æ¨¡å‹è¾“å‡ºåˆ†æ:")
            print(f"   è¾“å‡ºå¼ é‡: shape{actions_tensor.shape}, device={actions_tensor.device}")
            print(f"   è¾“å‡ºèŒƒå›´: [{actions_tensor.min().item():.3f}, {actions_tensor.max().item():.3f}]")
            print(f"   æ¨¡å‹æ¨ç†è€—æ—¶: {model_time:.2f}ms")
            
            # åå¤„ç†
            postprocess_start = time.time()
            actions = self._postprocess_output(actions_tensor)
            postprocess_time = (time.time() - postprocess_start) * 1000
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = (time.time() - start_time) * 1000  # ms
            
            # è°ƒè¯•ä¿¡æ¯ï¼šè¾“å‡ºåŠ¨ä½œåˆ†æ
            print(f"ğŸ“Š åŠ¨ä½œåºåˆ—åˆ†æ:")
            print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape} (chunk_size={CHUNK_SIZE}, action_dim={ACTION_DIM})")
            print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min():.3f}, {actions.max():.3f}]")
            print(f"   å…³èŠ‚åŠ¨ä½œèŒƒå›´: [{actions[:, :7].min():.3f}, {actions[:, :7].max():.3f}]")
            print(f"   å¤¹çˆªåŠ¨ä½œèŒƒå›´: [{actions[:, 7].min():.3f}, {actions[:, 7].max():.3f}]")
            print(f"   åå¤„ç†è€—æ—¶: {postprocess_time:.2f}ms")
            
            # æ€§èƒ½åˆ†è§£ç»Ÿè®¡
            print(f"â±ï¸ æ€§èƒ½åˆ†è§£ç»Ÿè®¡:")
            print(f"   é¢„å¤„ç†: {preprocess_time:.2f}ms ({preprocess_time/total_time*100:.1f}%)")
            print(f"   æ¨¡å‹æ¨ç†: {model_time:.2f}ms ({model_time/total_time*100:.1f}%)")
            print(f"   åå¤„ç†: {postprocess_time:.2f}ms ({postprocess_time/total_time*100:.1f}%)")
            print(f"   æ€»è€—æ—¶: {total_time:.2f}ms")
            
            # æ›´æ–°ç»Ÿè®¡
            self.inference_count += 1
            self.total_inference_time += total_time
            
            return actions, total_time
            
        except Exception as e:
            raise RuntimeError(f"æ¨ç†å¤±è´¥: {e}")
    
    def _preprocess_input(self, qpos: np.ndarray, image: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:
        """é¢„å¤„ç†è¾“å…¥æ•°æ®"""
        # éªŒè¯è¾“å…¥
        if qpos.shape != (STATE_DIM,):
            raise ValueError(f"qposç»´åº¦é”™è¯¯: æœŸæœ›({STATE_DIM},), å®é™…{qpos.shape}")
        
        # qposæ•°æ®æ ‡å‡†åŒ–
        if self.pre_process is not None:
            qpos_normalized = self.pre_process(qpos)
            print(f"   qposæ ‡å‡†åŒ–: {qpos[:3]} -> {qpos_normalized[:3]} (å‰3ä¸ªå…³èŠ‚)")
        else:
            qpos_normalized = qpos
            print(f"   qposæœªæ ‡å‡†åŒ–: {qpos[:3]} (å‰3ä¸ªå…³èŠ‚)")
        
        expected_image_shape = (ACTConstants.IMAGE_HEIGHT, ACTConstants.IMAGE_WIDTH, 3)
        if image.shape != expected_image_shape:
            raise ValueError(f"imageç»´åº¦é”™è¯¯: æœŸæœ›{expected_image_shape}, å®é™…{image.shape}")
        
        # æ£€æŸ¥qposæ•°æ®åˆç†æ€§
        if np.any(np.isnan(qpos)) or np.any(np.isinf(qpos)):
            raise ValueError(f"qposåŒ…å«éæ³•æ•°å€¼: NaN={np.any(np.isnan(qpos))}, Inf={np.any(np.isinf(qpos))}")
        
        # æ£€æŸ¥å›¾åƒæ•°æ®åˆç†æ€§
        if not (0 <= image.min() <= image.max() <= 255):
            print(f"âš ï¸ å›¾åƒåƒç´ å€¼å¼‚å¸¸: èŒƒå›´[{image.min()}, {image.max()}], åº”è¯¥åœ¨[0, 255]")
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦ä¸ºRGBæ ¼å¼
        if image.dtype != np.uint8:
            print(f"âš ï¸ å›¾åƒæ•°æ®ç±»å‹å¼‚å¸¸: {image.dtype}, åº”è¯¥æ˜¯uint8")
        
        # è½¬æ¢ä¸ºå¼ é‡
        qpos_tensor = torch.from_numpy(qpos_normalized).float().unsqueeze(0).to(self.device)  # (1, 8)
        
        # å›¾åƒé¢„å¤„ç†: (H, W, 3) -> (1, 1, 3, H, W) å¹¶ä½¿ç”¨ImageNetæ ‡å‡†åŒ–
        # å…ˆè½¬æ¢ä¸º [0, 1] èŒƒå›´
        image_normalized = image.astype(np.float32) / 255.0  # [0, 1]
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).to(self.device)  # (3, H, W)
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ImageNetæ ‡å‡†åŒ–
        normalize_transform = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        image_tensor = normalize_transform(image_tensor)  # ImageNetæ ‡å‡†åŒ–
        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, 3, H, W)
        
        return qpos_tensor, image_tensor
    
    def _postprocess_output(self, actions_tensor: torch.Tensor) -> np.ndarray:
        """åå¤„ç†è¾“å‡ºæ•°æ®"""
        # æ£€æŸ¥è¾“å‡ºå¼ é‡æ•°æ®åˆç†æ€§
        if torch.any(torch.isnan(actions_tensor)) or torch.any(torch.isinf(actions_tensor)):
            print(f"âš ï¸ æ¨¡å‹è¾“å‡ºåŒ…å«éæ³•æ•°å€¼!")
            print(f"   NaN: {torch.any(torch.isnan(actions_tensor)).item()}")
            print(f"   Inf: {torch.any(torch.isinf(actions_tensor)).item()}")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        actions_raw = actions_tensor.cpu().numpy()  # (1, 100, 8)
        actions_raw = actions_raw[0]  # (100, 8)
        
        # åŠ¨ä½œåæ ‡å‡†åŒ–
        if self.post_process is not None:
            # é€æ­¥åæ ‡å‡†åŒ–æ¯ä¸ªåŠ¨ä½œ
            actions = np.zeros_like(actions_raw)
            for i in range(actions_raw.shape[0]):
                actions[i] = self.post_process(actions_raw[i])
            print(f"   åŠ¨ä½œåæ ‡å‡†åŒ–: åŸå§‹[{actions_raw[0, :3]}] -> ç»“æœ[{actions[0, :3]}]")
        else:
            actions = actions_raw
            print(f"   åŠ¨ä½œæœªåæ ‡å‡†åŒ–: {actions[0, :3]}")
        
        # éªŒè¯è¾“å‡º
        expected_shape = (CHUNK_SIZE, ACTION_DIM)
        if actions.shape != expected_shape:
            raise RuntimeError(f"è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{actions.shape}")
        
        # æ£€æŸ¥åŠ¨ä½œæ•°æ®åˆç†æ€§
        if np.any(np.isnan(actions)) or np.any(np.isinf(actions)):
            raise RuntimeError(f"åŠ¨ä½œè¾“å‡ºåŒ…å«éæ³•æ•°å€¼!")
        
        # æ£€æŸ¥åŠ¨ä½œèŒƒå›´æ˜¯å¦åˆç†ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        joint_actions = actions[:, :7]  # å…³èŠ‚åŠ¨ä½œ
        gripper_actions = actions[:, 7]   # å¤¹çˆªåŠ¨ä½œ
        
        # æ£€æŸ¥å…³èŠ‚åŠ¨ä½œèŒƒå›´ (ä¸€èˆ¬åœ¨ -3 åˆ° 3 ä¹‹é—´)
        joint_max = np.abs(joint_actions).max()
        if joint_max > 5.0:
            print(f"âš ï¸ å…³èŠ‚åŠ¨ä½œèŒƒå›´å¼‚å¸¸: {joint_max:.3f} > 5.0")
        elif joint_max > 3.0:
            print(f"â„¹ï¸ å…³èŠ‚åŠ¨ä½œèŒƒå›´è¾ƒå¤§: {joint_max:.3f} (æ­£å¸¸èŒƒå›´: Â±3.0)")
        
        # æ£€æŸ¥å¤¹çˆªåŠ¨ä½œèŒƒå›´ (åæ ‡å‡†åŒ–ååº”è¯¥åœ¨ 0 åˆ° 0.08 ä¹‹é—´)
        gripper_min, gripper_max = gripper_actions.min(), gripper_actions.max()
        if self.stats is not None:
            # åæ ‡å‡†åŒ–åçš„åˆç†èŒƒå›´
            if gripper_min < -0.01 or gripper_max > 0.09:
                print(f"âš ï¸ å¤¹çˆªåŠ¨ä½œèŒƒå›´å¼‚å¸¸: [{gripper_min:.3f}, {gripper_max:.3f}] (æœŸæœ›èŒƒå›´: [0, 0.08])")
            elif gripper_min < 0 or gripper_max > 0.08:
                print(f"â„¹ï¸ å¤¹çˆªåŠ¨ä½œèŒƒå›´è¾ƒå¤§: [{gripper_min:.3f}, {gripper_max:.3f}] (æ­£å¸¸èŒƒå›´: [0, 0.08])")
        else:
            # æœªæ ‡å‡†åŒ–æ•°æ®çš„èŒƒå›´æ£€æŸ¥
            if gripper_min < -0.1 or gripper_max > 0.1:
                print(f"âš ï¸ å¤¹çˆªåŠ¨ä½œèŒƒå›´å¼‚å¸¸: [{gripper_min:.3f}, {gripper_max:.3f}] (æœªæ ‡å‡†åŒ–æ•°æ®)")
        
        # æ£€æŸ¥åŠ¨ä½œåºåˆ—çš„è¿ç»­æ€§ï¼ˆç¬¬ä¸€æ­¥å’Œåç»­æ­¥éª¤çš„å·®å¼‚ï¼‰
        if actions.shape[0] > 1:
            action_diff = np.abs(actions[1:] - actions[:-1]).max()
            if action_diff > 1.0:
                print(f"â„¹ï¸ åŠ¨ä½œåºåˆ—å˜åŒ–è¾ƒå¤§: æœ€å¤§æ­¥é—´å·®å¼‚ {action_diff:.3f}")
        
        return actions
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if self.inference_count == 0:
            avg_time = 0.0
        else:
            avg_time = self.total_inference_time / self.inference_count
        
        return {
            "is_loaded": self.is_loaded,
            "device": str(self.device),
            "model_path": str(self.model_path),
            "inference_count": self.inference_count,
            "total_time_ms": self.total_inference_time,
            "avg_time_ms": avg_time,
            "parameters_M": self._count_parameters() if self.is_loaded else 0.0,
            "data_normalization": self.stats is not None,
            "qpos_stats": {
                "mean": self.stats['qpos_mean'].tolist() if self.stats else None,
                "std": self.stats['qpos_std'].tolist() if self.stats else None
            } if self.stats else None,
            "action_stats": {
                "mean": self.stats['action_mean'].tolist() if self.stats else None,
                "std": self.stats['action_std'].tolist() if self.stats else None
            } if self.stats else None
        }
    
    def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.inference_count = 0
        self.total_inference_time = 0.0
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

def test_inference_engine():
    """æµ‹è¯•æ¨ç†å¼•æ“"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ACTæ¨ç†å¼•æ“...")
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = ACTInferenceEngine()
    
    # åŠ è½½æ¨¡å‹
    if not engine.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return False
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    qpos = np.random.randn(8) * 0.1  # å°å¹…åº¦éšæœºä½ç½®
    image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)  # éšæœºå›¾åƒ
    
    print(f"ğŸ¯ æ‰§è¡Œæ¨ç†æµ‹è¯•...")
    
    try:
        # æ‰§è¡Œæ¨ç†
        actions, inference_time = engine.predict(qpos, image)
        
        print(f"âœ… æ¨ç†æˆåŠŸ!")
        print(f"   è¾“å…¥: qpos{qpos.shape}, image{image.shape}")
        print(f"   è¾“å‡º: actions{actions.shape}")
        print(f"   è€—æ—¶: {inference_time:.2f}ms")
        print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min():.3f}, {actions.max():.3f}]")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = engine.get_stats()
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_inference_engine()
    print(f"\n{'ğŸ‰ æµ‹è¯•é€šè¿‡!' if success else 'âŒ æµ‹è¯•å¤±è´¥!'}")
