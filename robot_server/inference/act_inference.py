#!/usr/bin/env python3
"""
ACTæ¨¡å‹æ¨ç†æ¨¡å—
åŸºäºè®­ç»ƒå¥½çš„Frankaå•è‡‚ACTæ¨¡å‹è¿›è¡Œæ¨ç†
"""

import torch
import numpy as np
import time
import os
import sys
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import traceback

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ACTç»„ä»¶
current_dir = Path(__file__).parent
robot_server_dir = current_dir.parent
sys.path.append(str(robot_server_dir))

from act_algo_train.policy import ACTPolicy
from act_algo_train.franka_constants import STATE_DIM, ACTION_DIM, CHUNK_SIZE

# å¸¸é‡å®šä¹‰
class ACTConstants:
    STATE_DIM = 8
    ACTION_DIM = 8  
    CHUNK_SIZE = 100
    IMAGE_HEIGHT = 480
    IMAGE_WIDTH = 640

class ACTConfig:
    @staticmethod
    def get_model_config():
        return {
            'lr': 1e-5,
            'num_queries': 100,
            'kl_weight': 10,
            'hidden_dim': 512,
            'dim_feedforward': 3200,
            'lr_backbone': 1e-5,
            'backbone': 'resnet18',
            'enc_layers': 4,
            'dec_layers': 7,
            'nheads': 8,
            'camera_names': ['top']
        }
    
    @staticmethod
    def get_default_model_path():
        return "/home/wujielin/CascadeProjects/data/act_training/models/checkpoints/franka_pick_place"

class ACTInferenceEngine:
    """ACTæ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: Optional[str] = None, device: str = "auto"):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            device: æ¨ç†è®¾å¤‡ï¼Œ"auto"è‡ªåŠ¨é€‰æ‹©ï¼Œ"cuda"æˆ–"cpu"
        """
        self.model = None
        self.device = self._setup_device(device)
        self.model_path = model_path or ACTConfig.get_default_model_path()
        self.is_loaded = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        print(f"ğŸ¤– ACTæ¨ç†å¼•æ“åˆå§‹åŒ–")
        print(f"   æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"   æ¨ç†è®¾å¤‡: {self.device}")
    
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®æ¨ç†è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                print(f"ğŸš€ æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                device = "cpu"
                print(f"ğŸ’» ä½¿ç”¨CPUæ¨ç†")
        
        return torch.device(device)
    
    def load_model(self) -> bool:
        """
        åŠ è½½ACTæ¨¡å‹
        
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ“¦ å¼€å§‹åŠ è½½ACTæ¨¡å‹...")
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
            model_file = self._find_latest_checkpoint()
            if model_file is None:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_path}")
            
            print(f"ğŸ“„ ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: {model_file.name}")
            
            # åˆ›å»ºæ¨¡å‹é…ç½®
            config = ACTConfig.get_model_config()
            
            # åˆ›å»ºç­–ç•¥æ¨¡å‹
            self.model = ACTPolicy(config)
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(model_file, map_location=self.device)
            
            # æ£€æŸ¥checkpointæ ¼å¼å¹¶åŠ è½½æƒé‡
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"ğŸ“Š æ£€æŸ¥ç‚¹æ ¼å¼: æ ‡å‡†æ ¼å¼")
            else:
                # ç›´æ¥æ˜¯æ¨¡å‹æƒé‡
                self.model.load_state_dict(checkpoint)
                print(f"ğŸ“Š æ£€æŸ¥ç‚¹æ ¼å¼: æƒé‡æ ¼å¼")
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åˆ°æŒ‡å®šè®¾å¤‡
            self.model.eval()
            if hasattr(self.model, 'to'):
                self.model = self.model.to(self.device)
            
            # éªŒè¯æ¨¡å‹
            self._validate_model()
            
            self.is_loaded = True
            print(f"âœ… ACTæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   å‚æ•°æ•°é‡: {self._count_parameters():.2f}M")
            print(f"   è¾“å…¥ç»´åº¦: qpos({STATE_DIM}) + image({ACTConstants.IMAGE_HEIGHT}Ã—{ACTConstants.IMAGE_WIDTH}Ã—3)")
            print(f"   è¾“å‡ºç»´åº¦: actions({CHUNK_SIZE}Ã—{ACTION_DIM})")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _find_latest_checkpoint(self) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        model_path = Path(self.model_path)
        
        if model_path.is_file():
            return model_path
        elif model_path.is_dir():
            # æŸ¥æ‰¾ç›®å½•ä¸­çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            ckpt_files = list(model_path.glob('policy_epoch_*.ckpt'))
            if not ckpt_files:
                return None
            
            # æŒ‰epochæ’åºï¼Œå–æœ€æ–°çš„
            ckpt_files.sort(key=lambda x: int(x.stem.split('_')[2]))
            return ckpt_files[-1]
        else:
            return None
    
    def _validate_model(self):
        """éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½"""
        print(f"ğŸ§ª éªŒè¯æ¨¡å‹åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 1
        qpos = torch.randn(batch_size, STATE_DIM).to(self.device)
        image = torch.randn(batch_size, 1, 3, ACTConstants.IMAGE_HEIGHT, ACTConstants.IMAGE_WIDTH).to(self.device)
        
        # æµ‹è¯•æ¨ç†
        with torch.no_grad():
            actions = self.model(qpos, image)
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        expected_shape = (batch_size, CHUNK_SIZE, ACTION_DIM)
        if actions.shape != expected_shape:
            raise RuntimeError(f"æ¨¡å‹è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{actions.shape}")
        
        print(f"âœ… æ¨¡å‹éªŒè¯é€šè¿‡: è¾“å‡ºå½¢çŠ¶{actions.shape}")
    
    def _count_parameters(self) -> float:
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡(M)"""
        if self.model is None:
            return 0.0
        
        total_params = sum(p.numel() for p in self.model.parameters())
        return total_params / 1e6
    
    def predict(self, qpos: np.ndarray, image: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        æ‰§è¡ŒACTæ¨ç†
        
        Args:
            qpos: æœºå™¨äººå…³èŠ‚ä½ç½® (8,)
            image: RGBå›¾åƒ (H, W, 3)
            
        Returns:
            Tuple[np.ndarray, float]: (åŠ¨ä½œåºåˆ—(100, 8), æ¨ç†è€—æ—¶(ms))
        """
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        start_time = time.time()
        
        try:
            # æ•°æ®é¢„å¤„ç†
            qpos_tensor, image_tensor = self._preprocess_input(qpos, image)
            
            # æ¨ç†
            with torch.no_grad():
                actions_tensor = self.model(qpos_tensor, image_tensor)
            
            # åå¤„ç†
            actions = self._postprocess_output(actions_tensor)
            
            # è®¡ç®—è€—æ—¶
            inference_time = (time.time() - start_time) * 1000  # ms
            
            # æ›´æ–°ç»Ÿè®¡
            self.inference_count += 1
            self.total_inference_time += inference_time
            
            return actions, inference_time
            
        except Exception as e:
            raise RuntimeError(f"æ¨ç†å¤±è´¥: {e}")
    
    def _preprocess_input(self, qpos: np.ndarray, image: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:
        """é¢„å¤„ç†è¾“å…¥æ•°æ®"""
        # éªŒè¯è¾“å…¥
        if qpos.shape != (STATE_DIM,):
            raise ValueError(f"qposç»´åº¦é”™è¯¯: æœŸæœ›({STATE_DIM},), å®é™…{qpos.shape}")
        
        expected_image_shape = (ACTConstants.IMAGE_HEIGHT, ACTConstants.IMAGE_WIDTH, 3)
        if image.shape != expected_image_shape:
            raise ValueError(f"imageç»´åº¦é”™è¯¯: æœŸæœ›{expected_image_shape}, å®é™…{image.shape}")
        
        # è½¬æ¢ä¸ºå¼ é‡
        qpos_tensor = torch.from_numpy(qpos).float().unsqueeze(0).to(self.device)  # (1, 8)
        
        # å›¾åƒé¢„å¤„ç†: (H, W, 3) -> (1, 1, 3, H, W) å¹¶å½’ä¸€åŒ–
        image_normalized = image.astype(np.float32) / 255.0  # [0, 1]
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0).unsqueeze(0).to(self.device)  # (1, 1, 3, H, W)
        
        return qpos_tensor, image_tensor
    
    def _postprocess_output(self, actions_tensor: torch.Tensor) -> np.ndarray:
        """åå¤„ç†è¾“å‡ºæ•°æ®"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        actions = actions_tensor.cpu().numpy()  # (1, 100, 8)
        
        # ç§»é™¤batchç»´åº¦
        actions = actions[0]  # (100, 8)
        
        # éªŒè¯è¾“å‡º
        expected_shape = (CHUNK_SIZE, ACTION_DIM)
        if actions.shape != expected_shape:
            raise RuntimeError(f"è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{actions.shape}")
        
        return actions
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if self.inference_count == 0:
            avg_time = 0.0
        else:
            avg_time = self.total_inference_time / self.inference_count
        
        return {
            "is_loaded": self.is_loaded,
            "device": str(self.device),
            "model_path": str(self.model_path),
            "inference_count": self.inference_count,
            "total_time_ms": self.total_inference_time,
            "avg_time_ms": avg_time,
            "parameters_M": self._count_parameters() if self.is_loaded else 0.0
        }
    
    def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.inference_count = 0
        self.total_inference_time = 0.0
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

def test_inference_engine():
    """æµ‹è¯•æ¨ç†å¼•æ“"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ACTæ¨ç†å¼•æ“...")
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = ACTInferenceEngine()
    
    # åŠ è½½æ¨¡å‹
    if not engine.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return False
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    qpos = np.random.randn(8) * 0.1  # å°å¹…åº¦éšæœºä½ç½®
    image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)  # éšæœºå›¾åƒ
    
    print(f"ğŸ¯ æ‰§è¡Œæ¨ç†æµ‹è¯•...")
    
    try:
        # æ‰§è¡Œæ¨ç†
        actions, inference_time = engine.predict(qpos, image)
        
        print(f"âœ… æ¨ç†æˆåŠŸ!")
        print(f"   è¾“å…¥: qpos{qpos.shape}, image{image.shape}")
        print(f"   è¾“å‡º: actions{actions.shape}")
        print(f"   è€—æ—¶: {inference_time:.2f}ms")
        print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min():.3f}, {actions.max():.3f}]")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = engine.get_stats()
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_inference_engine()
    print(f"\n{'ğŸ‰ æµ‹è¯•é€šè¿‡!' if success else 'âŒ æµ‹è¯•å¤±è´¥!'}")
