#!/bin/bash
# ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨é…ç½®è„šæœ¬
# åŸºäºagentlaceåè®®çš„æœåŠ¡å™¨ç«¯é…ç½®ï¼Œå……åˆ†åˆ©ç”¨RTX 4090 GPU

set -e

echo "========================================="
echo "ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨é…ç½®"
echo "ç‰ˆæœ¬: v2.0 - AgentLaceåè®®"
echo "========================================="

# è¯»å–é…ç½®æ–‡ä»¶
CONFIG_FILE="robot_config.yaml"
if [[ ! -f "$CONFIG_FILE" ]]; then
    echo "âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $CONFIG_FILE"
    exit 1
fi

echo "ğŸ“‹ è¯»å–é…ç½®æ–‡ä»¶: $CONFIG_FILE"

# ä½¿ç”¨pythonè§£æYAMLé…ç½®
python3 -c "
import yaml
import sys

try:
    with open('$CONFIG_FILE', 'r') as f:
        config = yaml.safe_load(f)
    
    # ç½‘ç»œé…ç½®
    network = config['network']
    print(f'export ACT_SERVER_IP=\"{network[\"server_ip\"]}\"')
    print(f'export ACT_SERVER_PORT=\"{network[\"server_port\"]}\"')
    print(f'export ACT_BROADCAST_PORT=\"{network[\"broadcast_port\"]}\"')
    print(f'export ACT_PROTOCOL_VERSION=\"{network[\"protocol_version\"]}\"')
    
    # GPUé…ç½®
    server_opt = config['server_optimization']
    print(f'export GPU_DEVICE=\"{server_opt[\"gpu_device\"]}\"')
    print(f'export MAX_BATCH_SIZE=\"{server_opt[\"max_batch_size\"]}\"')
    print(f'export INFERENCE_WORKERS=\"{server_opt[\"inference_workers\"]}\"')
    print(f'export MAX_GPU_MEMORY=\"{server_opt[\"max_gpu_memory\"]}\"')
    print(f'export PYTORCH_CUDA_ALLOC_CONF=\"{server_opt[\"pytorch_cuda_alloc_conf\"]}\"')
    print(f'export CUDA_LAUNCH_BLOCKING=\"{str(server_opt[\"cuda_launch_blocking\"]).lower()}\"')
    
    # ACTæ¨¡å‹é…ç½®
    act_model = config['act_model']
    print(f'export ACT_CHUNK_SIZE=\"{act_model[\"chunk_size\"]}\"')
    print(f'export ACT_HIDDEN_DIM=\"{act_model[\"hidden_dim\"]}\"')
    print(f'export ACT_DIM_FEEDFORWARD=\"{act_model[\"dim_feedforward\"]}\"')
    print(f'export ACT_KL_WEIGHT=\"{act_model[\"kl_weight\"]}\"')
    print(f'export ACT_NUM_QUERIES=\"{act_model[\"num_queries\"]}\"')
    print(f'export ACT_INFERENCE_WORKERS=\"{server_opt[\"inference_workers\"]}\"')
    
    # è·¯å¾„é…ç½®
    paths = config['paths']
    print(f'export ACT_MODEL_DIR=\"{paths[\"model_dir\"]}\"')
    print(f'export ACT_LOG_DIR=\"{paths[\"log_dir\"]}\"')
    
except Exception as e:
    print(f'echo \"âŒ é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}\"', file=sys.stderr)
    sys.exit(1)
" > config_temp.sh

# æ£€æŸ¥é…ç½®è§£ææ˜¯å¦æˆåŠŸ
if [[ $? -ne 0 ]]; then
    echo "âŒ é…ç½®æ–‡ä»¶è§£æå¤±è´¥"
    rm -f config_temp.sh
    exit 1
fi

# åŠ è½½é…ç½®å˜é‡
source config_temp.sh
rm -f config_temp.sh

echo "ğŸ”§ AgentLaceåè®®é…ç½®:"
echo "   æœåŠ¡å™¨IP: $ACT_SERVER_IP"
echo "   ä¸»ç«¯å£: $ACT_SERVER_PORT (REQ-REP)"  
echo "   å¹¿æ’­ç«¯å£: $ACT_BROADCAST_PORT (PUB-SUB)"
echo "   åè®®ç‰ˆæœ¬: $ACT_PROTOCOL_VERSION"
echo ""
echo "âš¡ æœåŠ¡å™¨æ€§èƒ½é…ç½®:"
echo "   GPUè®¾å¤‡: RTX 4090 (24GB)"
echo "   æ¨ç†æ‰¹æ¬¡: $ACT_BATCH_SIZE"
echo "   å·¥ä½œè¿›ç¨‹: $ACT_INFERENCE_WORKERS"
echo "   æ¨¡å‹è·¯å¾„: $ACT_MODEL_DIR"
echo ""

# æ£€æŸ¥condaç¯å¢ƒ
if ! command -v conda &> /dev/null; then
    echo "âŒ Condaæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Minicondaæˆ–Anaconda"
    exit 1
fi

# æ¿€æ´»æˆ–åˆ›å»ºalohaç¯å¢ƒ
echo "ğŸ”§ é…ç½®Pythonç¯å¢ƒ..."
source $HOME/miniconda3/etc/profile.d/conda.sh
if ! conda env list | grep -q "aloha"; then
    echo "åˆ›å»ºaloha condaç¯å¢ƒ..."
    conda create -n aloha python=3.9 -y
fi

conda activate aloha

# æœåŠ¡å™¨ç«¯ä¾èµ–å®‰è£…
echo "ğŸ“¦ å®‰è£…æœåŠ¡å™¨ç«¯Pythonä¾èµ–..."
pip install --upgrade pip

# æ·±åº¦å­¦ä¹ æ¡†æ¶
pip install torch==2.0.0 torchvision==0.15.0 --index-url https://download.pytorch.org/whl/cu118

# åŸºç¡€ç§‘å­¦è®¡ç®—åŒ…
pip install numpy==1.24.3 scipy matplotlib seaborn
pip install gymnasium==1.1.1 opencv-python-headless
pip install pillow>=8.0.0 imageio

# Webæ¡†æ¶å’Œç½‘ç»œé€šä¿¡
pip install flask==2.3.2 requests==2.31.0
pip install websockets aiohttp

# é…ç½®æ–‡ä»¶å’Œæ•°æ®å¤„ç†
pip install pyyaml>=6.0 h5py wandb
pip install tqdm ipython

# æ¨¡å‹ç›¸å…³ä¾èµ–
pip install transformers==4.30.0
pip install einops timm

# AgentLaceåˆ†å¸ƒå¼æ¡†æ¶ä¾èµ–
pip install pyzmq>=24.0.0
pip install msgpack>=1.0.0
pip install cloudpickle>=2.0.0

echo "ğŸ§ª éªŒè¯æœåŠ¡å™¨ç«¯ç¯å¢ƒ..."
python3 -c "
import torch
print(f'âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'âœ“ CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'âœ“ GPUæ•°é‡: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'  - GPU{i}: {torch.cuda.get_device_name(i)}')

import numpy as np
print(f'âœ“ NumPyç‰ˆæœ¬: {np.__version__}')

import yaml, flask
print('âœ“ é…ç½®å’ŒWebæ¡†æ¶å¯¼å…¥æˆåŠŸ')

import zmq
print(f'âœ“ ZeroMQç‰ˆæœ¬: {zmq.zmq_version()}')
print('âœ“ AgentLaceé€šä¿¡åè®®å°±ç»ª')
"

# åˆ›å»ºæœåŠ¡å™¨ç«¯ç¯å¢ƒæ¿€æ´»è„šæœ¬
echo "ğŸ“„ åˆ›å»ºæœåŠ¡å™¨ç«¯ç¯å¢ƒè„šæœ¬..."
cat > activate_server_env.sh << 'EOF'
#!/bin/bash
# æœåŠ¡å™¨ç«¯ç¯å¢ƒæ¿€æ´»è„šæœ¬

source $HOME/miniconda3/etc/profile.d/conda.sh
conda activate aloha

# AgentLaceåè®®ç¯å¢ƒå˜é‡
export ACT_SERVER_IP="10.16.49.124"
export ACT_SERVER_PORT="5555" 
export ACT_BROADCAST_PORT="5556"

# GPUä¼˜åŒ–é…ç½®
export CUDA_VISIBLE_DEVICES="0"
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:1024"
export CUDA_LAUNCH_BLOCKING="0"

# é¡¹ç›®è·¯å¾„é…ç½®
export FRANKA_ACT_ROOT=$(pwd)
export PYTHONPATH=$FRANKA_ACT_ROOT:$FRANKA_ACT_ROOT/robot_server:$FRANKA_ACT_ROOT/communication:$PYTHONPATH

echo "ğŸ–¥ï¸  ACTæ¨ç†æœåŠ¡å™¨ç¯å¢ƒå·²æ¿€æ´»"
echo "   - Condaç¯å¢ƒ: aloha"
echo "   - GPUè®¾å¤‡: RTX 4090 (24GB)"
echo "   - æœåŠ¡å™¨åœ°å€: $ACT_SERVER_IP:$ACT_SERVER_PORT"
echo "   - é¡¹ç›®æ ¹ç›®å½•: $FRANKA_ACT_ROOT"
echo "   - PyTorchç‰ˆæœ¬: $(python -c 'import torch; print(torch.__version__)')"
EOF

chmod +x activate_server_env.sh

# AgentLaceåè®®è§„èŒƒé…ç½®æ–‡ä»¶
echo "ğŸ“‹ åˆ›å»ºAgentLaceåè®®é…ç½®..."
cat > agentlace_config.py << 'EOF'
#!/usr/bin/env python3
"""
AgentLaceåè®®é…ç½® - ACTåˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿ
å®šä¹‰å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´çš„é€šä¿¡åè®®å’Œæ•°æ®æ ¼å¼
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List
import numpy as np

# ============================================================================
# AgentLaceåè®®é…ç½®
# ============================================================================

@dataclass
class ACTTrainerConfig:
    """ACTæ¨ç†æœåŠ¡å™¨AgentLaceé…ç½®"""
    port_number: int = 5555              # ä¸»é€šä¿¡ç«¯å£(REQ-REP)
    broadcast_port: int = 5556           # å¹¿æ’­ç«¯å£(PUB-SUB) 
    request_types: List[str] = field(default_factory=lambda: [
        "inference",                     # æ¨ç†è¯·æ±‚
        "model_update",                 # æ¨¡å‹æ›´æ–°
        "server_status"                 # æœåŠ¡å™¨çŠ¶æ€æŸ¥è¯¢
    ])
    rate_limit: int = 1000              # è¯·æ±‚é€Ÿç‡é™åˆ¶(req/s)
    version: str = "0.0.2"              # åè®®ç‰ˆæœ¬

# ============================================================================
# æ•°æ®æ ¼å¼å®šä¹‰
# ============================================================================

# æ¨ç†è¯·æ±‚æ•°æ®æ ¼å¼
INFERENCE_REQUEST_FORMAT = {
    "type": "inference",
    "payload": {
        "qpos": [],                     # List[float] - 8ç»´å…³èŠ‚ä½ç½®
        "qvel": [],                     # List[float] - 8ç»´å…³èŠ‚é€Ÿåº¦
        "image": "",                    # str - base64ç¼–ç çš„å›¾åƒ
        "timestamp": 0.0,               # float - æ—¶é—´æˆ³
        "request_id": ""                # str - è¯·æ±‚ID
    }
}

# æ¨ç†å“åº”æ•°æ®æ ¼å¼  
INFERENCE_RESPONSE_FORMAT = {
    "success": True,                    # bool - æ¨ç†æ˜¯å¦æˆåŠŸ
    "actions": [],                      # List[List[float]] - 100x8åŠ¨ä½œåºåˆ—
    "latency_ms": 0.0,                 # float - æ¨ç†å»¶è¿Ÿ(æ¯«ç§’)
    "error_msg": "",                   # str - é”™è¯¯ä¿¡æ¯(å¦‚æœæœ‰)
    "request_id": ""                   # str - å¯¹åº”çš„è¯·æ±‚ID
}

# æœåŠ¡å™¨çŠ¶æ€å“åº”æ ¼å¼
SERVER_STATUS_FORMAT = {
    "status": "running",               # str - æœåŠ¡å™¨çŠ¶æ€
    "model_loaded": True,              # bool - æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    "gpu_memory_mb": 0,                # int - GPUå†…å­˜ä½¿ç”¨(MB)
    "inference_count": 0,              # int - ç´¯è®¡æ¨ç†æ¬¡æ•°
    "avg_latency_ms": 0.0,            # float - å¹³å‡æ¨ç†å»¶è¿Ÿ
    "uptime_seconds": 0                # float - è¿è¡Œæ—¶é—´(ç§’)
}

# ============================================================================
# å¸¸é‡å®šä¹‰
# ============================================================================

class ACTConstants:
    """ACTç³»ç»Ÿå¸¸é‡"""
    
    # ç½‘ç»œé…ç½®
    SERVER_IP = "10.16.49.124"
    DEFAULT_PORT = 5555
    DEFAULT_BROADCAST_PORT = 5556
    
    # Frankaæœºå™¨äººè§„æ ¼
    STATE_DIM = 8                      # 7å…³èŠ‚ + 1å¤¹çˆª
    ACTION_DIM = 8                     # åŒçŠ¶æ€ç»´åº¦
    CHUNK_SIZE = 100                   # ACTé¢„æµ‹æ­¥æ•°
    
    # å›¾åƒè§„æ ¼
    IMAGE_HEIGHT = 480
    IMAGE_WIDTH = 640
    IMAGE_CHANNELS = 3
    
    # æ§åˆ¶å‚æ•°
    CONTROL_FREQ = 50                  # Hz
    DT = 0.02                         # æ§åˆ¶é—´éš”(ç§’)

EOF

# åˆ›å»ºæ¨ç†æœåŠ¡å™¨å¯åŠ¨è„šæœ¬
echo "ğŸš€ åˆ›å»ºæ¨ç†æœåŠ¡å™¨å¯åŠ¨è„šæœ¬..."
cat > start_inference_server.sh << 'EOF'
#!/bin/bash
# ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨å¯åŠ¨è„šæœ¬
# åŸºäºAgentLaceåè®®çš„åˆ†å¸ƒå¼æ¨ç†æ¶æ„

set -e

# å‚æ•°è®¾ç½®
SERVER_IP=${1:-"0.0.0.0"}      # AgentLaceæœåŠ¡å™¨ç›‘å¬IP
SERVER_PORT=${2:-5555}         # AgentLaceæœåŠ¡å™¨ç«¯å£
TASK=${3:-"franka_pick_place"}  # ä»»åŠ¡åç§°ï¼ˆæ¨¡å‹è·¯å¾„ï¼‰
GPU_ID=${4:-0}                 # GPUè®¾å¤‡ID

echo "========================================="
echo "ğŸš€ å¯åŠ¨ ACT åˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨..."
echo "AgentLaceåœ°å€: $SERVER_IP:$SERVER_PORT"
echo "æ¨¡å‹ä»»åŠ¡: $TASK"  
echo "GPUè®¾å¤‡: $GPU_ID"
echo "========================================="

# æ¿€æ´»ç¯å¢ƒ
source ./activate_server_env.sh

# è®¾ç½®GPUè®¾å¤‡
export CUDA_VISIBLE_DEVICES=$GPU_ID

# æ£€æŸ¥GPUå¯ç”¨æ€§
if command -v nvidia-smi &> /dev/null; then
    echo "ğŸ–¥ï¸  GPUçŠ¶æ€æ£€æŸ¥ï¼š"
    nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader,nounits
else
    echo "âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUæ¨ç†"
fi

# æ£€æŸ¥AgentLace
python -c "
try:
    from agentlace.trainer import TrainerServer, TrainerConfig
    print('âœ… AgentLaceå¯¼å…¥æˆåŠŸ')
except ImportError as e:
    print(f'âŒ AgentLaceå¯¼å…¥å¤±è´¥: {e}')
    exit(1)
"

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
MODEL_PATH="/home/wujielin/CascadeProjects/data/act_training/models/checkpoints/$TASK"
if [[ ! -d "$MODEL_PATH" ]]; then
    echo "âŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ $MODEL_PATH"
    exit 1
else
    echo "âœ… æ¨¡å‹è·¯å¾„éªŒè¯: $MODEL_PATH"
    ls -la "$MODEL_PATH"/*.ckpt | head -3
fi

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs
mkdir -p /tmp/act_server_logs

# å¯åŠ¨AgentLaceåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨
echo "ğŸŒ å¯åŠ¨AgentLaceæ¨ç†æœåŠ¡å™¨..."
cd ../robot_server/communication/
python act_server.py \
    --port $SERVER_PORT \
    --model_path "$MODEL_PATH" \
    --config ../communication/robot_config.yaml \
    2>&1 | tee ../../logs/act_inference_server_$(date +%Y%m%d_%H%M%S).log &

SERVER_PID=$!

echo "========================================="
echo "âœ… ACT åˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨å¯åŠ¨å®Œæˆï¼"
echo ""
echo "ğŸŒ æœåŠ¡å™¨ä¿¡æ¯ï¼š"
echo "- AgentLaceè¿›ç¨‹ID: $SERVER_PID"  
echo "- ç›‘å¬åœ°å€: $SERVER_IP:$SERVER_PORT"
echo "- æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "- æ—¥å¿—æ–‡ä»¶: logs/act_inference_server_*.log"
echo ""
echo "ğŸ§ª æµ‹è¯•å‘½ä»¤ï¼š"
echo "- æœ¬åœ°æµ‹è¯•: python3 ../../test_act_client.py"
echo "- æ€§èƒ½æµ‹è¯•: python3 ../robot_server/test_server.py"
echo "- ç½‘ç»œæµ‹è¯•: nc -zv $SERVER_IP $SERVER_PORT"
echo ""
echo "ğŸ“Š ç›‘æ§å‘½ä»¤ï¼š"
echo "- æŸ¥çœ‹è¿›ç¨‹: ps aux | grep act_server"
echo "- æŸ¥çœ‹æ—¥å¿—: tail -f ../../logs/act_inference_server_*.log"
echo "- GPUç›‘æ§: watch -n 1 nvidia-smi"
echo "- ç½‘ç»œè¿æ¥: netstat -tulpn | grep $SERVER_PORT"
echo ""
echo "ğŸ›‘ åœæ­¢æœåŠ¡å™¨: Ctrl+C æˆ– kill $SERVER_PID"
echo "========================================="

# ç­‰å¾…ç”¨æˆ·ä¸­æ–­
trap 'echo "ğŸ›‘ æ­£åœ¨åœæ­¢AgentLaceæ¨ç†æœåŠ¡å™¨..."; kill $SERVER_PID; exit 0' INT

# ä¿æŒè„šæœ¬è¿è¡Œï¼Œæ˜¾ç¤ºå®æ—¶çŠ¶æ€
echo "ğŸ”„ AgentLaceæ¨ç†æœåŠ¡å™¨è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C åœæ­¢..."
while kill -0 $SERVER_PID 2>/dev/null; do
    sleep 15
    echo "$(date '+%H:%M:%S'): ğŸŸ¢ AgentLaceæ¨ç†æœåŠ¡å™¨è¿è¡Œä¸­ (PID: $SERVER_PID)"
done

echo "ğŸ›‘ AgentLaceæ¨ç†æœåŠ¡å™¨å·²åœæ­¢"

EOF

chmod +x start_inference_server.sh

echo "âœ… ACTåˆ†å¸ƒå¼æ¨ç†æœåŠ¡å™¨ç¯å¢ƒé…ç½®å®Œæˆï¼"
echo ""
echo -e "${GREEN}ğŸš€ ä½¿ç”¨æ–¹æ³•ï¼š${NC}"
echo "1. æ¿€æ´»ç¯å¢ƒ: source ./activate_server_env.sh"
echo "2. å¯åŠ¨æ¨ç†æœåŠ¡å™¨: ./start_inference_server.sh [ç«¯å£] [æ¨¡å‹è·¯å¾„]"
echo ""
echo -e "${BLUE}ğŸ“‹ ç³»ç»Ÿä¿¡æ¯ï¼š${NC}"
echo "   - åè®®: AgentLace v0.0.2 (ZeroMQ)"
echo "   - æœåŠ¡å™¨IP: $ACT_SERVER_IP"
echo "   - é»˜è®¤ç«¯å£: $ACT_SERVER_PORT"
echo "   - GPUä¼˜åŒ–: RTX 4090 (24GBæ˜¾å­˜)"
echo "   - æ¨ç†æ‰¹æ¬¡: æœ€å¤§16ä¸ªå¹¶å‘è¯·æ±‚"
echo ""
echo -e "${YELLOW}âš ï¸ æ³¨æ„äº‹é¡¹ï¼š${NC}"
echo "   - ç¡®ä¿æ¨¡å‹å·²è®­ç»ƒå®Œæˆ (policy_epoch_*.ckpt)"
echo "   - å®¢æˆ·ç«¯éœ€è¦è¿æ¥åˆ° $ACT_SERVER_IP:$ACT_SERVER_PORT"  
echo "   - æ•°æ®æ ¼å¼: qpos(8ç»´) + image(480Ã—640Ã—3) â†’ actions(100Ã—8)"
echo ""
