#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæœåŠ¡å™¨ç«¯æµ‹è¯•Demo
æµ‹è¯•æ¨¡å‹åŠ è½½ã€GPUæ¨ç†å’Œç½‘ç»œé€šä¿¡
"""

import torch
import torch.nn as nn
import socket
import threading
import json
import time
import numpy as np
from typing import Dict, Any

class SimpleACTModel(nn.Module):
    """ç®€åŒ–ç‰ˆACTæ¨¡å‹ï¼Œç”¨äºæµ‹è¯•"""
    def __init__(self, obs_dim=7, action_dim=8, chunk_size=100, hidden_dim=256):
        super().__init__()
        self.chunk_size = chunk_size
        
        # ç®€åŒ–çš„ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ç®€åŒ–çš„è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim * chunk_size)
        )
        
    def forward(self, obs):
        # obs: [batch_size, obs_dim]
        encoded = self.encoder(obs)
        decoded = self.decoder(encoded)
        # é‡å¡‘ä¸ºåŠ¨ä½œåºåˆ—: [batch_size, chunk_size, action_dim]
        actions = decoded.view(-1, self.chunk_size, 8)
        return actions

class SimpleTrainingServer:
    """ç®€åŒ–ç‰ˆè®­ç»ƒæœåŠ¡å™¨"""
    
    def __init__(self, port=5555, device='cuda'):
        self.port = port
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = SimpleACTModel().to(self.device)
        self.running = False
        
        print(f"ğŸ–¥ï¸  åˆå§‹åŒ–æœåŠ¡å™¨ - è®¾å¤‡: {self.device}")
        
    def start_server(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        self.running = True
        
        # åˆ›å»ºsocket
        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_socket.bind(('0.0.0.0', self.port))
        server_socket.listen(5)
        
        print(f"ğŸš€ æœåŠ¡å™¨å¯åŠ¨åœ¨ç«¯å£ {self.port}")
        
        while self.running:
            try:
                client_socket, addr = server_socket.accept()
                print(f"ğŸ“¡ å®¢æˆ·ç«¯è¿æ¥: {addr}")
                
                # å¯åŠ¨å®¢æˆ·ç«¯å¤„ç†çº¿ç¨‹
                client_thread = threading.Thread(
                    target=self.handle_client, 
                    args=(client_socket, addr)
                )
                client_thread.daemon = True
                client_thread.start()
                
            except Exception as e:
                print(f"âŒ æœåŠ¡å™¨é”™è¯¯: {e}")
                break
                
        server_socket.close()
        
    def handle_client(self, client_socket, addr):
        """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚"""
        try:
            while self.running:
                # æ¥æ”¶æ•°æ®
                data = client_socket.recv(4096).decode('utf-8')
                if not data:
                    break
                    
                # è§£æè¯·æ±‚
                request = json.loads(data)
                
                if request['type'] == 'predict':
                    # æ‰§è¡Œæ¨ç†
                    response = self.predict(request['data'])
                    
                elif request['type'] == 'ping':
                    # å¿ƒè·³æ£€æµ‹
                    response = {'type': 'pong', 'timestamp': time.time()}
                    
                else:
                    response = {'type': 'error', 'message': 'Unknown request type'}
                
                # å‘é€å“åº”
                response_str = json.dumps(response) + '\n'
                client_socket.send(response_str.encode('utf-8'))
                
        except Exception as e:
            print(f"âŒ å®¢æˆ·ç«¯å¤„ç†é”™è¯¯ {addr}: {e}")
        finally:
            client_socket.close()
            print(f"ğŸ“¡ å®¢æˆ·ç«¯æ–­å¼€: {addr}")
    
    def predict(self, obs_data):
        """æ¨¡å‹æ¨ç†"""
        try:
            # è½¬æ¢è§‚æµ‹æ•°æ®
            obs = torch.tensor(obs_data['qpos'], dtype=torch.float32).unsqueeze(0).to(self.device)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                start_time = time.time()
                actions = self.model(obs)
                inference_time = time.time() - start_time
            
            # è½¬æ¢ç»“æœ
            actions_np = actions.cpu().numpy()[0]  # [chunk_size, action_dim]
            
            return {
                'type': 'prediction',
                'actions': actions_np.tolist(),
                'inference_time': inference_time,
                'chunk_size': actions_np.shape[0],
                'action_dim': actions_np.shape[1]
            }
            
        except Exception as e:
            return {
                'type': 'error',
                'message': f'æ¨ç†é”™è¯¯: {str(e)}'
            }

def main():
    print("========================================")
    print("Franka ACT ç®€åŒ–ç‰ˆæœåŠ¡å™¨æµ‹è¯•")
    print("========================================")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        device = 'cuda'
    else:
        print("âš ï¸  ä½¿ç”¨CPUè¿è¡Œ")
        device = 'cpu'
    
    # å¯åŠ¨æœåŠ¡å™¨
    server = SimpleTrainingServer(port=5555, device=device)
    
    try:
        server.start_server()
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å™¨åœæ­¢")
        server.running = False

if __name__ == '__main__':
    main()
