# **SE(3)上机器人机械手统一力-阻抗控制的几何公式**

## **摘要**

在本文中，我们提出了一个在 SE(3) 流形上的阻抗控制框架，该框架能够实现力跟踪同时保证无源性。在统一力-阻抗控制（UFIC）和我们之前关于几何阻抗控制（GIC）的工作基础上，我们开发了几何统一力-阻抗控制（GUFIC），以使用微分几何视角在控制器公式中考虑 SE(3) 流形结构。与 UFIC 的情况一样，GUFIC 利用能量罐增强来同时实现力跟踪和阻抗控制，以保证机械手相对于外部力的无源性。这确保了末端执行器与不确定环境保持安全接触交互并跟踪期望的交互力。此外，我们通过引入速度场和力场解决了 UFIC 公式中的非因果实现问题。由于其在 SE(3) 上的公式，所提出的 GUFIC 继承了 GIC 所期望的 SE(3) 不变性和等变性特性，这有助于提高将学习算法纳入控制律的机器学习应用的样本效率。所提出的控制律在需要跟踪 SE(3) 轨迹（包括位置和方向）同时在表面上施加力的场景下在仿真环境中进行了验证。代码可在 [https://github.com/Joohwan-Seo/GUFIC\_mujoco](https://github.com/Joohwan-Seo/GUFIC_mujoco) 获取。

## **I. 引言**

阻抗控制 \[1\] 自引入以来，已被用作涉及与未知环境交互的机器人操作任务的主要控制方案。阻抗控制通常与操作空间公式 \[2\] 结合使用，也广泛用于控制机械手的末端执行器。由于阻抗/导纳控制确保了与环境的安全交互，因此它被用作最近基于学习的策略中的低级控制律 \[3\]。

深度学习方法的最新进展在执行实际任务方面显示出令人印象深刻的结果。特别是，基于模仿学习的策略，例如行为转换器 \[4\]、扩散策略 \[5\]、动作分块转换器 \[6\] 及其变体，通过从视觉输入生成期望的末端执行器轨迹而取得了成功。

尽管这些工作在实际任务中取得了成功，但提供期望姿态的策略可能不足以满足高精度和接触密集型任务的需求。由于这一限制，最近的工作还提出了输出阻抗/导纳增益 \[7\]、\[8\] 或直接力曲线 \[9\]。尽管除了指定的姿态之外，改变增益或施加直接力可以提高接触密集型任务的性能，但此类方法在保证稳定性方面带来了挑战。在任务执行期间改变阻抗/导纳增益通常被称为可变阻抗控制 \[10\]。然而，从控制理论的角度来看，当增益改变时，原始静态增益阻抗/导纳控制器的稳定性结果无法得到保证 \[11\]。

机器人与环境交互的稳定性分析通常与无源性概念嵌套在一起。这是因为无源性定理可以为整个机器人/环境交互动力学提供稳定性保证，如果机器人的闭环系统被设计为无源的，因为环境是严格无源的 \[12\]、\[13\]、\[14\]。\[15\]、\[16\] 说明了在自优化运动器械的学习控制器设计和实现中闭环无源性的使用，其中人机安全交互是必不可少的。其他基于无源性的控制设计示例包括 \[17\]、\[18\]、\[19\]。

在 \[20\] 中，提出了统一力-阻抗控制（UFIC），作为机器人机械手在执行任务时使用阻抗控制并施加期望力来保持与环境接触的一种方法。通过能量罐增强，UFIC 控制确保了闭环系统的无源性。然而，UFIC 没有考虑机械手末端执行器姿态描述中固有的 SE(3) 流形结构，而只是将姿态未对准误差视为笛卡尔向量。 \[20\] 中的另一个缺点是，为了使用能量罐增强来建立阻抗控制项的无源性，必须定义一个修改后的期望速度，然后必须对其进行积分以生成新的轨迹。这使得下一步期望速度的更新变得复杂，并可能导致控制过程中的因果关系崩溃。这将在第三节 B.3 中进一步讨论。

我们之前关于几何阻抗控制（GIC）的工作 \[21\]、\[22\] 建立了一个统一的框架，用于使用微分几何控制末端执行器的位置和方向。GIC 框架的一个显著优点是，如 \[23\] 所示，在控制结构中考虑 SE(3) 流形结构会导致学习策略中的 SE(3) 不变性和等变性，这大大提高了深度学习模型中样本效率和对分布外数据的鲁棒性。SE(3) 等变性在提高视觉操作学习模型样本效率和对分布外数据的鲁棒性方面的优势也已在最近得到证明 \[24\]、\[25\]、\[26\]、\[27\]。

在本文中，我们提出了机器人机械手在 SE(3) 流形上的统一力-阻抗控制方案的几何公式，即几何统一力-阻抗控制（GUFIC）。我们论文的主要贡献可以总结如下：

1. 与 \[20\] 不同，\[20\] 将平移和方向误差分别作为向量空间中的元素进行处理，我们充分整合了 SE(3) 的流形结构，并在 SE(3) 李群结构内一致地处理了平移和方向误差。  
2. 我们引入了一个时间相关的速度场来编码任务并推导轨迹。因此，对速度场的任何调整都保留了因果关系，这解决了 \[20\] 中的实现困难。  
3. 由于我们遵循 \[23\] 的公式，因此生成的控制律是 SE(3) 等变的，从而为学习操作任务提供了进一步的学习可迁移性和样本效率优势。  
4. 从结合力的学习操作任务的角度来看，控制公式的无源行为将进一步提供接触稳定性等优势。

本文组织如下。在第二节中，我们简要介绍了李群和李代数、机械手动力学和几何阻抗控制（GIC）的初步背景。接下来，我们在第三节中介绍了我们的 GUFIC 框架并展示了其无源性。仿真结果将在第四节中展示，验证了 SE(3) 上的力跟踪和运动跟踪特性。结论将在第五节中提供。

## **II. 预备知识**

### **A. 李群与李代数**

机械手末端执行器的配置可以由其位置和方向定义。在许多其他方向表示（如欧拉角和四元数）中，我们对位置和方向的统一表示感兴趣，即特殊欧几里得群 SE(3)。我们通过从末端执行器坐标系 {e} 到固定（惯性）空间坐标系 {s} 的齐次坐标变换矩阵 gse​ 来描述末端执行器的配置：

gse​=\[R0​p1​\]∈SE(3)(1)  
其中 R 是旋转矩阵，R∈SO(3)，且 p∈R3。请注意，(1) 中的这种矩阵表示被称为齐次矩阵表示。我们将省略下标 s，因为空间坐标系可以普遍视为恒等式。此外，为了符号紧凑性，我们将省略末端执行器当前配置的下标 e，即 gse​=g，除非另有说明。我们使用 g=(p,R) 来表示符号紧凑性。

SE(3) 的李代数 se(3) 可以表示为：

ξ^​=\[ω^0​v0​\]∈se(3)∀ξ=\[vω​\]∈R6,ω∈R3,ω^∈so(3)(2)  
有关机器人机械手李群的详细信息，请参阅 \[28\]、\[29\]。另请注意，我们使用 \[21\] 中定义的标准 hat-map 和 vee-map 符号。我们还注意到 se(3) 与 R6 同构。

### **B. 机械手动力学**

不失一般性，我们考虑旋转关节臂的机械手动力学：

M(q)q¨​+C(q,q˙​)q˙​+G(q)=T+Te​(3)  
其中 q=\[q1​,⋯,qn​\]T∈S，S≜S1×⋯×S1（重复 n 次）是关节坐标，M(q)∈Rn×n 是对称正定惯量矩阵，C(q,q˙​)∈Rn×n 是科里奥利矩阵，G(q)∈Rn 是由于重力引起的力矩项，T∈Rn 是控制输入关节力矩，而 Te​∈Rn 是由于外部扰动引起的关节力矩。相对于末端执行器本体坐标系的 SE(3) 操作空间动力学如下 \[22\]：

M\~(q)V˙b+C\~(q,q˙​)Vb+G\~(q)=F+Fe​(4)  
其中 M\~(q)=Jb​(q)−TM(q)Jb​(q)−1，C\~(q,q˙​)=Jb​(q)−T(C(q,q˙​)−M(q)Jb​(q)−1J˙)Jb​(q)−1，G\~(q)=Jb​(q)−TG(q)，F=Jb​(q)−TT，Fe​=Jb​(q)−TTe​，且 Jb​(q) 是末端执行器本体坐标系雅可比矩阵，即：

Vb=\[vbωb​\]=Jb​(q)q˙​(5)  
其中上标 b 表示向量在本体坐标系中定义，vb 和 ωb 分别是平移速度和方向速度。

请注意，在我们的论文 \[21\] 中，我们使用符号 T 来表示 se(3)∗ 末端执行器空间中的力矩/力，但我们将使用符号 F，遵循 \[20\] 中的符号，以显示与 UFIC 公式直接关联。此外，我们将 Fe​ 定义为环境施加在机器人上的外部力矩/力，它与期望力矩/力相反。我们还注意到，为了紧凑性，我们将经常省略关节坐标和关节速度的依赖性，除非为了清晰起见有必要，例如 M\~=M\~(q) 和 C\~=C\~(q,q˙​)。

### **C. 几何阻抗控制**

在 \[21\]、\[22\] 中，提出了几何阻抗控制（GIC）来控制末端执行器在 SE(3) 流形上的位置和方向。总而言之，GIC 控制律如下所示：

Fi​=M\~V˙d∗​+C\~Vd∗​+G\~−fG​(g,gd​)−Kd​eV​(6)  
其中 g=(p,R) 是当前末端执行器配置，gd​=(pd​,Rd​) 是期望配置，g,gd​∈SE(3)，且 Kd​∈R6×6 是对称正定阻尼矩阵。此外，eV​=Vb−Vd∗​，Vd∗​=Adged​​Vdb​，ged​=g−1gd​，且 Ad:SE(3)×R6→R6 是一个大伴随映射，给出为：

Adg​=\[R0​p^​RR​\](7)  
我们使用 Vd∗​ 星号表示法，表示期望的本体坐标系速度已转换为当前配置。我们将对广义向量或余向量使用相同的表示法。 (6) 中的 fG​ 项是 SE(3) 中的弹性力：

fG​(g,gd​)=\[fp​(g,gd​)fR​(g,gd​)​\]=\[RTRd​Kp​RdT​(p−pd​)(KR​RdT​R−RTRd​KR​)∨​\](8)  
其中 Kp​,KR​∈R3×3 分别是平移和旋转动力学的对称正刚度矩阵。

GIC 控制律 (6) 是使用 SE(3) 上的势能和动能函数制定的。我们将选择从李群导出的势函数，而不是其他可能的势能函数选择 \[22\]。因此，误差势函数 P(t,q) 和误差动能函数 K(t,q,q˙​) 定义如下：

P(t,q)=tr(KR​(I−RdT​R))+21​(p−pd​)TRd​Kp​RdT​(p−pd​)(9)K(t,q,q˙​)=21​eVT​M\~eV​(10)  
需要注意的是，GIC 律是基于以下假设制定的：

**假设 1** (\[21\])：末端执行器位于区域 D⊂SE(3) 中，使得雅可比 Jb​ 满秩。此外，机械手末端执行器和期望轨迹位于可达集 R 中，即：

p(q)∈R={p(q)∣∀q∈S}∈R3  
并且期望轨迹也是连续可微的。基于 GIC 律，我们开发了一种学习可变阻抗控制来解决钉孔任务问题 \[23\]，表明所得方法是 SE(3) 等变的。然而，当阻抗增益变化时，原始控制律的稳定性特性可能不再有效，如 \[11\] 中所述。事实上，控制系统在与外部环境交互时的稳定行为可以通过能量无源性特性 \[30\]、\[12\] 更好地理解。另一方面，从装配任务的角度来看，可变阻抗控制的最终目标是适应外部力 \[31\]，这意味着将可变阻抗控制公式化为允许直接力反馈的控制可能更有利。

因此，下一节介绍了几何统一力-阻抗控制，这是一种增强了力控制的 GIC，它通过能量罐保持了无源性。

## **III. 几何统一力-阻抗控制**

在本节中，我们将重新制定原始的统一力-阻抗控制（UFIC）\[20\]，以便充分考虑末端执行器 SE(3) 的流形结构。通过将其公式化为 GIC 形式，当它与学习增强时，所得的控制框架将具有 SE(3) 等变性 \[23\]。

### **A. 朴素力跟踪控制律**

在本节中，我们将几何阻抗控制 Fi​ 在 (6) 中与力跟踪控制 Ff​ 进行增强，以便总输入控制力矩/力为：

F=Fi​+Ff​(11)  
其中 Tf​=JbT​Ff​，且：

Ff​=−kp​(−Fe​−Fd​)−kd​dtd​(−Fe​​−Fd​)−ki​∫(−Fe​(τ)−Fd​(τ))dτ+Fd​(12)  
如同 \[20\] 中，我们使用 Fe​=Fe​(t) 表示力/力矩传感器输出，不将其与环境施加在机器人上的实际外部力矩/力 Fe​ 区分开来。我们还将 Fd​=Fd​(t,g) 定义为期望力场。关于力场公式的详细信息将在后面的章节中进一步阐述。然而，如下文所示，朴素力-阻抗控制律 (11) 是非无源的。

1. 无源性分析：当满足以下条件时，控制系统对于对 (Vb,Fe​) 或供应率 (Vb)TFe​ 是无源的：

S˙≤(Vb)TFe​(13)

其中 S∈R\>0​ 是正定储能函数。注意施加在环境上的力是 −Fe​，环境的无源性条件给出为：

S˙env​≤(Vb)T(−Fe​)(14)  
即，它应该在对 (Vb,−Fe​) 下是无源的。带有朴素几何力-阻抗控制律的误差动力学可以写成：

M\~e˙V​+C\~eV​+Kd​eV​+fG​−Ff​−Fe​=0(15)  
遵循 \[21\] 中的公式，我们将使用 SE(3) 中的势能函数和动能函数之和作为储能函数，即：

S(t,q,q˙​)=K(t,q,q˙​)+P(t,q)(16)  
其中 K 和 P 在 (10) 中定义。利用 \[21\]、\[22\] 中 fGT​eV​=P˙ 的事实，可以进一步证明：

dtdS​=eVT​M\~e˙V​+21​eVT​M\~˙eV​+fGT​eV​=eVT​(−C\~eV​−fG​+Ff​+Fe​)+21​eVT​M\~˙eV​=≥021​eVT​(M\~˙−2C\~)eV​​​−eVT​Kd​eV​+eVT​Ff​+eVT​Fe​≤(Vb)TFe​+(Vb)TFf​−(Vd∗​)T(Ff​+Fe​)(17)  
由于项 (Vb)TFf​ 和 (Vd∗​)T(Ff​+Fe​) 的符号未确定，因此无法保证控制系统的无源性。

### **B. 无源控制律公式**

为了使控制系统闭环无源，我们将通过能量罐增强来整合力跟踪和阻抗控制器的能量存储，如 \[20\] 中所述。

1. 力跟踪控制器的能量罐增强：首先，对于端口 (Vb,Ff​)，与力控制相关的能量罐 Tf​ 首先定义为：

Tf​=21​xtf2​xtf​=0(18)

能量罐状态动力学为：

x˙tf​=−xtf​βf​​γf​(Vb)TFf​+xtf​αf​​(γf​−1)(Vb)TFf​(19)  
其中：

γf​={10​if(Vb)TFf​\<0otherwise​βf​={10​if Tf​≤Tu,fotherwise​αf​=⎩⎨⎧​121​(1−cos(δT,f​Tf​−Tl,f​​π))0​if Tf​≥Tl,f​+δT,f​if Tl,f​+δT,f​≥Tf​≥Tl,f​otherwise​  
其中 Tu,f 和 Tl,f​ 分别表示力跟踪控制能量罐的上限和下限，δT,f​ 是能量罐下限的裕度，以实现平滑切换行为。γf​ 的目的是指示力跟踪律 Ff​ 是否处于违反无源性的方向，βf​ 是为了防止能量罐溢出。根据能量罐水平和 γf​ 值，力跟踪律 Ff​ 修改为：

Ff′​=(γf​+αf​(1−γf​))Ff​(20)  
稍后将阐明，(20) 的作用是保证力跟踪端口 (Vb,Ff′​) 的无源性。

2. 阻抗控制器的能量罐增强：其次，对于端口 (Vd∗​,−(Ff′​+Fe​))，与阻抗控制相关的能量罐 Ti​ 定义为：

Ti​=21​xti2​xti​=0(21)

能量罐状态动力学给出为：

x˙ti​=xti​βi​​(γi​(Vd∗​)T(Ff′​+Fe​)+(eV′​)TKd​eV′​)+xti​αi​​(1−γi​)(Vd∗​)T(Ff′​+Fe​)(22)  
其中：

eV′​=Vb−(γi​+αi​(1−γi​))Vd∗​=Vb−(Vd∗​)′(23)  
且：

(Vd∗​)′=(γi​+α(1−γi​))Vd∗​(24)  
是修改后的期望速度。γi​、βi​ 和 αi​ 分别定义为：

γi​={10​if(Vd∗​)T(Ff′​+Fe​)\>0otherwise​βi​={10​if Ti​≤Tu,iotherwise​αi​=⎩⎨⎧​121​(1−cos(δT,i​Ti​−Tl,i​​π))0​if Ti​≥Tl,i​+δT,i​if Tl,i​+δT,i​≥Ti​≥Tl,i​otherwise​  
其中 Tu,i 和 Tl,i​ 分别表示阻抗控制能量罐的上限和下限，δT,i​ 是阻抗控制能量罐的平滑裕度，类似于力跟踪控制的裕度。

3. 速度场和力场公式：实现修改后的期望速度 (Vd∗​)′ 时出现一个微妙的挑战；相应地更新相应的信号 (V˙d∗​)′ 和 gd′​。虽然原始工作 \[20\] 建议适当地积分和微分修改后的速度信号，但朴素的实现很容易导致因果关系问题，需要仔细的工程设计以确保精确更新。具体而言，在当前时间步积分修改后的速度会产生更新后的轨迹，但下一步的期望速度无法以因果方式直接计算。为了解决这个问题，我们建议采用速度场 \[12\] 的概念。形式上，速度场 Vd∗​(t,g) 定义为从当前时间 t 和当前姿态 g∈SE(3) 到李代数 se(3) 上的向量的映射，确保 g(t)V\~d∗​(t,g)∈Tg​SE(3)。因此，我们提出的速度场表示为 V\~d∗​:R≥0​×SE(3)→se(3)，明确依赖于时间和姿态，而不仅仅是时间。

a) 速度场：让我们首先将原始时间轨迹表示为 g​d​(t)=(p​d​(t),Rd​(t))，以及它们对应的期望本体坐标系速度为 Vdb​(t)。我们还将省略时间依赖性以避免混乱，例如 g​d​=g​d​(t)。遵循 \[12\] 的公式，结合 SE(3) 的流形感触，从期望轨迹 g​d​ 和 V^db​=g​d−1​g​˙​d​ 得到时变速度场 V^d∗​(t,g) 为：

V^d∗​(t,g)=g​ed​V^db​(t)g​ed−1​+ζ∇1​Ψ(g,g​d​)(25)  
其中 g​ed​=g−1g​d​，Ψ(g,g​d​) 是一个误差函数，给出为：

Ψ(g,g​d​)=21​∣∣I4​−gd−1​g∣∣F2​=tr(I−RdT​R)+21​∣∣p−pd​∣∣22​(26)  
这可以作为 \[21\] 中提出的 SE(3) 上的距离度量。ζ∈R\>0​ 是一个正标量增益，∇1​ 表示函数第一个参数的梯度。还可以注意到，根据 \[28\]（参见第 2.4 章），

V^d∗​=g​ed​V^db​g​ed−1​⟺Vd∗​=Adg​ed​​Vdb​(27)  
此外，\[21\] 中显示：

∇1​Ψ(g,g​d​)=e^G​(g,g​d​)(28)  
其中 eG​(g,g​d​) 是 \[23\]、\[21\] 中提出的几何一致误差向量（GCEV），给出为：

eG​(g,gd​)=\[RT(p−pd​)(RdT​R−RTRd​)∨​\](29)  
有关 SE(3) 的更多结果，请参见 \[32\]、\[33\]、\[34\]，有关 SO(3) 的结果，请参见 \[35\]。

现在我们已经配备了速度场 Vd∗​(t,g)，速度修改律 (23) 可以自由应用。使用修改后的速度场进行轨迹跟踪 (V\~d∗​(t,g))′，修改后的期望配置 gd′​(t) 可以通过积分得到：

g˙​d′​=gd′​(V^db​)′(30)  
其中 (Vdb​)′=Adged−1​​(Vd∗​)′，这可以通过 gd′​(t+Δt)≅gd​(t)′exp(V^db​Δt)′ 进行离散化。

我们强调，由于期望姿态现在是从速度场导出的，因此修改速度场和获取期望姿态可以在不损失因果关系的情况下执行。

速度场 Vd∗​ 的时间导数需要直接计算。附录 A 中显示了 Vd​ 和 Vd∗​ 的完整计算。

b) 力场：由于期望速度是用场公式化的，期望力也可以用场结构类似地公式化。为了使其具有最大的自由度，我们将期望力公式化为时变速度场，即 Fd​=Fd​(t,g)∈Tg∗​SE(3)，其中 Tg∗​SE(3) 表示 Tg​SE(3) 的对偶空间。基于此公式，可以通过取消对当前配置 g 的依赖来使用时间相关的期望力，反之亦然。另一个考虑是是否在流形 SE(3) 上的点 g 或 gd​ 上定义期望力。当期望力在期望姿态 gd​ 的对偶空间 Tgd​∗​SE(3) 上表示时，需要应用适当的坐标变换（对偶伴随映射）Adged−1​∗​ \[23\]，以便：

Fd∗​(t)=Adged−1​∗​Fd​(t)=Adged​−1​Fd​(t)∈Tg∗​SE(3)(31)  
在本文中，我们只考虑简单情况，即 Fd​ 在空间和时间上是常数。

4. 最终几何统一力-阻抗控制器：使用从速度场计算的修改设定点，修改后的阻抗控制器公式如下：

Fi′​=M\~(V˙d∗​)′+C\~(Vd∗​)′+G\~−fG​(g,gd′​)−Kd​eV′​(32)

更新后的储能函数现在为：

S=21​(eV′​)TM\~eV′​+P(g,gd′​)(33)  
综合所有，最终的 GUFIC 如下所示：

T=JbT​F′其中F′=Ff′​+Fi′​(34)  
其中 Ff′​ 和 Fi′​ 分别在 (20) 和 (32) 中显示。本文的主要定理如下：

**定理 1** (GUFIC 的无源性)：假设假设 1 成立。考虑具有动力学 (3) 和 GUFIC 控制律 (34) 的机器人机械手。那么，闭环系统对于通道 (Vb,Fe​) 是无源的，相对于储能函数 (33)。

证明：使用控制律 (34)，修改后的误差动力学读作：

M\~V˙b+C\~Vb+G\~=Ff′​+Fi′​+Fe​(35)⇒M\~e˙V′​+C\~eV′​+Kd​eV′​+fG​(g,gd′​)=Ff′​+Fe​(36)  
结合 (32)、(20) 和 (26)，储能函数的时间导数为：

S˙=(eV′​)TM\~e˙V′​+21​(eV′​)TM\~˙eV′​+(eV′​)TfG​(g,gd′​)=(eV′​)T(−C\~eV′​−fG​(g,gd′​)−Kd​eV′​+Ff′​+Fe​)+21​M\~˙eV′​+(eV′​)TfG​(g,gd′​)=−(eV′​)TKd​eV′​+(eV′​)TFf′​+(eV′​)TFe​(37)  
通过增强能量罐 Ti​ 和 Tf​，总储能函数 Stot​ 现在为：

Stot​=S+Ti​+Tf​(38)  
Tf​=21​xtf2​ 和 Ti​=21​xti2​ 的时间导数如下：

dtd​(Tf​+Ti​)=xtf​x˙tf​+xti​x˙ti​=−βf​γf​(Vb)TFf​+αf​(γf​−1)(Vb)TFf​+βi​(γi​(Vd∗​)T(Ff′​+Fe​)+(eV′​)TKd​eV′​)+αi​(1−γi​)(Vd∗​)T(Ff′​+Fe​)(39)  
总储能函数的时间导数然后为：

S˙tot​=S˙+T˙f​+T˙i​=−(eV′​)TKd​eV′​+(Vb)TFe​+(Vb)TFf′​−(Vd∗​)T(Ff′​+Fe​)−βf​γf​(Vb)TFf​+αf​(γf​−1)(Vb)TFf​+βi​(γi​(Vd∗​)T(Ff′​+Fe​)+(eV′​)TKd​eV′​+αi​(1−γi​)(Vd∗​)T(Ff′​+Fe​)(40)  
经过一些代数运算，得出：

S˙tot​=≤0γf​(1−βf​)(Vb)TFf​​​+≤0(βi​−1)(eV′​)TKd​eV′​​​+≤0γi​(βi​−1)(Vd∗​)T(Ff′​+Fe​)​​+(Vb)TFe​(41)  
为了详细说明，我们可以研究无源性被违反的情况。对于端口 (Vb,Ff​)，γf​ 的值取决于无源性条件。当无源性条件满足时，即 (Vb)TFf​\<0，则 γf​=1，当无源性被违反时，则 γf​=0。 (41) 中与此端口相关的项是 γf​(1−βf​)(Vb)TFf​。由于 (1−βf​)≥0 始终满足（因为 βf​∈{0,1}），并且如果无源性被违反则 γf​=0，我们证明：

γf​(1−βf​)(Vb)TFf​≤0(42)  
在任何情况下都成立。

类似地，对于端口 (Vd∗​,−(Ff′​+Fe​))，γi​=1 仅在无源性条件满足时，否则为 0。由于 (βi​−1)≤0，可以验证项 (βi​−1)(eV′​)TKd​eV′​≤0 和 γi​(βi​−1)(Vd∗​)T(Ff′​+Fe​)≤0。结合这些结果，表明：

S˙tot​≤(Vb)TFe​(43)  
这证明了闭环系统是无源的。

**备注 1** (稳定性结果)：使用结果 (43)，可以证明系统的稳定性，类似于 \[20\]。通过李雅普诺夫直接法进行的证明可以使用增强的李雅普诺夫函数和 \[21\]、\[32\] 中的耦合项。稳定性分析的主要含义是：1. 系统渐近收敛到 gd′​，即修改后的设定点；2. 当 g 收敛到 gd′​ 时，力也将收敛到 Fd​=−Fe​。

**备注 2** (SE(3) 不变性和等变性)：所提出的 GUFIC 框架也是 SE(3) 不变的。对于与 GIC 相关的项，左不变性来自 \[23\] 中引理 1 的结果。力跟踪律和速度场及力跟踪律的修改都定义在附加到末端执行器的本体坐标系上，从而导致控制器具有 SE(3) 不变性。此外，由于 GUFIC 律满足 SE(3) 左不变性并定义在末端执行器本体坐标系上，因此控制律是 SE(3) 等变的（参见 \[23\] 中命题 2）。

**备注 3** (接触损失稳定)：原始能量罐增强型 UFIC 框架中的一个关键问题是当力和运动控制变得平行或共线时，尤其是在末端执行器失去接触期间，导致不希望的运动。尽管原始论文 \[20\] 提到这种情况可以通过耗尽能量罐来避免危险接触来处理，但这种方法可能不切实际。根据他们的建议，我们采用了一个控制器整形函数 ρ，它调节力跟踪控制律以保持无源性并管理潜在的接触损失情况。虽然我们实现了这个控制器整形函数，但对其性能的详细分析不是我们的主要贡献。此外，由于页面限制，我们省略了其细节，并请读者参阅 \[20\] 以深入讨论其有效性。

## **IV. 仿真结果**

我们在 Mujoco 环境 \[36\] 中实现了 GUFIC 控制律，因为它以处理接触相关仿真而闻名。Neuromeka 的 Indy7 机器人 \[37\] 被整合到 Mujoco 环境中。在本仿真研究中，我们展示了两种力控制场景：1. 机器人在施加力到表面时遵循圆形轨迹；2. 机器人在球体上遵循包含 SE(3) 运动的轨迹。

力-力矩传感器值已通过截止频率为 5Hz 的二阶低通滤波器。整个仿真和控制循环的采样频率为 1000Hz。

### **A. 场景 1：圆形轨迹**

在此场景中，机器人在施加力场时遵循圆形轨迹。作为基准控制器，我们将使用标准 GIC 控制器。对于标准阻抗控制器，要在表面上施加期望力，需要精确的表面几何信息或自适应律。在此场景中，我们假设 GIC 控制器没有精确信息，只有粗略猜测，导致力施加不正确。特别是，期望轨迹 g​d​(t)=(p​d​(t),Rd​(t)) 为：

p​d​(t)=​0.5+0.1cost0.1sint0.125​​(44)Rd​(t)=​010​100​00−1​​(45)  
请注意，表面中心位置给定为 psurface​=\[0.5,0.0,0.1308\]T，并且 GIC 和 GUFIC 的目标位置都在表面内部以建立适当的接触。GUFIC 的增益由 Kp​=diag(\[2000,2000,10\])，KR​=diag(\[2000,2000,2000\])，(kp​,ki​,kd​)=(1.0,0.5,4.0)，Kd​=500I6×6​ 给出。对于速度场生成，使用了 ζ=5。对于 GIC，除了 Kp​=diag(\[2500,2500,1500\]) 之外，使用了与 GUFIC 相同的增益值，并且没有使用 PID 增益进行力跟踪控制。能量罐的初始值选择为 10，下限 Tl,i​=Tl,f​=0.1，上限 Tu,i=Tu,f=20，缓冲 δT,i​=δT,f​=0.5。力场选择为 Fd​(t,g)=\[0,0,10,0,0,0\]T。

第一个场景的轨迹跟踪结果如图 2 所示。可以看出，两种方法都显示出几乎完美的轨迹跟踪结果。表面法向方向的力跟踪结果如图 3 所示。由于 GIC 仅具有关于表面的粗略信息，因此所需力的增益值未知，导致跟踪期望力时出现显著的稳态误差。任务执行期间的能量罐值如图 4 所示。由于任务执行期间能量罐值未耗尽，因此可以成功完成任务。

### **B. 场景 2：球体上的直线轨迹**

在此场景中，控制器尝试在球体上遵循轨迹，同时施加法向力。具体而言，期望轨迹 g​d​(t) 如下所示：

p​d​(t)=\[0.4,0+0.3sinθ(t),−0.1+0.3cosθ(t)\]T(46)  
其中：

Rd​(t)=​cos(−θ(t))0−sin(−θ(t))​010​sin(−θ(t))0cos(−θ(t))​​(47)  
θ(t)=−4π​+20π​t。请注意，我们使用了与场景 1 相同的增益值，除了力跟踪控制器的 PID 增益。在此场景中使用的增益为 (kp​,ki​,kd​)=(1.5,0.75,6.0)。阻抗罐的初始值为 90，上限 Tu,i 为 100。

第二个场景的轨迹跟踪结果如图 5 所示，显示了 x 和 y 方向的完美轨迹跟踪结果，但在 z 方向存在轻微的稳态误差，因为球体信息不完善。误差函数 (26) 如图 6 所示，显示了平移和旋转动力学上跟踪误差的收敛。力跟踪结果如图 7 所示，显示了 GUFIC 的收敛，但 GIC 存在显著误差。能量罐 Tf​ 和 Ti​ 如图 8 所示。阻抗控制罐 Ti​ 迅速减少，因为前馈速度 Vd∗​ 处于无源性违反方向。因此，我们将初始值 Ti​(0) 设置得比前一个场景高得多，以防止耗尽。正如 \[20\] 中已经指出的，当阻抗罐 Ti​ 耗尽时，修改后的期望速度场 (Vd∗​)′ 变为零，导致静态期望姿态，即 gd​(k+1)=gd​(k)。类似地，当力控制罐 Tf​ 耗尽时，修改后的力控制 (Ff​)′ 变为零。

## **V. 结论与未来工作**

本文提出了一个几何统一力-阻抗控制（GUFIC）框架，该框架充分利用 SE(3) 流形结构来实现鲁棒的力跟踪，同时确保无源性。所提出的方法通过为控制器增强能量罐来确保与不确定环境的安全交互。此外，速度场和力场的引入解决了早期框架中存在的非因果实现问题。通过微分几何方法公式化统一力-阻抗控制，控制设计继承了 SE(3) 不变性和等变性，提高了学习样本效率。仿真结果验证了 GUFIC 在执行复杂 SE(3) 运动方面的有效性，展示了其在保持期望力曲线的同时跟踪具有位置和方向变化的轨迹的能力。

尽管我们在本文中假设速度场和力场是预先定义和提供的，但一个有趣的问题是这些场是否可以从专家演示中学习。在这方面，我们未来的工作将侧重于通过使用等变学习方法进行模仿学习来学习速度场和力场，以便整个模型管道能够保证无源性和等变性。

## **附录**

### **A. 速度场 Vd∗​ 的时间导数**

所提出的速度场 (25) 读作：

V^d∗​(t,g)=g​ed​V^db​(t)g​ed−1​+ζe^G​(g,g​d​)=\[RTR˙d​RdT​R0​RTR˙d​RdT​(p−p​d​)+RTp​˙​d​0​\]−ζ\[RdT​R−RTRd​0​RT(p−p​d​)0​\]  
每个项的时间导数可以很容易地获得如下：

dtd​(RTR˙d​RdT​R)=−ω^bRTR˙d​RdT​R+RTR¨d​RdT​R+RTR˙d​R˙dT​R+RTR˙d​RdT​Rω^bdtd​(RTR˙d​RdT​(p−p​d​)+RTp​˙​d​)=−ω^bRTR˙d​RdT​(p−p​d​)+RTR¨d​RdT​(p−p​d​)+RTR˙d​R˙dT​(p−p​d​)+RTR˙d​RdT​(RTv−p​˙​d​)−ω^bRTp​˙​d​+RTp​¨​d​dtd​(RdT​R−RTRd​)=R˙dT​R+RdT​Rω^b+ω^bRTRd​−RTR˙d​dtd​(RT(p−p​d​))=−ω^bRT(p−p​d​)+v−RTp​˙​d​  
请注意，Vb=\[(vb)T,(ωb)T\]T 是从 Vb=Jb​(q)q˙​ 获得的，并且使用了 R˙=Rω^b 和 p˙​=R˙Tv。另请注意，Rd​ 和 p​d​ 的时间导数可以从轨迹信号中获得。